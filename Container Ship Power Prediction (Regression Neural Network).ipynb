{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Nama Kapal</th>\n",
       "      <th>Flag State</th>\n",
       "      <th>Jenis Kapal</th>\n",
       "      <th>DWT</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>EHP ME</th>\n",
       "      <th>No ME</th>\n",
       "      <th>MCR ME</th>\n",
       "      <th>PME</th>\n",
       "      <th>...</th>\n",
       "      <th>Vapp</th>\n",
       "      <th>Vavg</th>\n",
       "      <th>Margin</th>\n",
       "      <th>Att. EEXI</th>\n",
       "      <th>Reference Line</th>\n",
       "      <th>Reduction Factor</th>\n",
       "      <th>Req. EEXI</th>\n",
       "      <th>Req - Att</th>\n",
       "      <th>Att/Req</th>\n",
       "      <th>Judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACACIA</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>Oil/Chemical Tanker</td>\n",
       "      <td>13565</td>\n",
       "      <td>13565</td>\n",
       "      <td>4900</td>\n",
       "      <td>1</td>\n",
       "      <td>4900</td>\n",
       "      <td>3675.00</td>\n",
       "      <td>...</td>\n",
       "      <td>13.144406</td>\n",
       "      <td>13.578394</td>\n",
       "      <td>0.678920</td>\n",
       "      <td>12.359438</td>\n",
       "      <td>11.730367</td>\n",
       "      <td>0.119563</td>\n",
       "      <td>10.327855</td>\n",
       "      <td>-2.031583</td>\n",
       "      <td>1.196709</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACADIA PARK</td>\n",
       "      <td>LIBERIA</td>\n",
       "      <td>Oil/Chemical Tanker</td>\n",
       "      <td>19996</td>\n",
       "      <td>19996</td>\n",
       "      <td>4900</td>\n",
       "      <td>1</td>\n",
       "      <td>4900</td>\n",
       "      <td>3675.00</td>\n",
       "      <td>...</td>\n",
       "      <td>12.486842</td>\n",
       "      <td>13.865005</td>\n",
       "      <td>0.693250</td>\n",
       "      <td>9.693722</td>\n",
       "      <td>9.706713</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>7.765856</td>\n",
       "      <td>-1.927866</td>\n",
       "      <td>1.248249</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ABRAO COCHIN</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>Container</td>\n",
       "      <td>23623</td>\n",
       "      <td>23623</td>\n",
       "      <td>15785</td>\n",
       "      <td>1</td>\n",
       "      <td>15785</td>\n",
       "      <td>11838.75</td>\n",
       "      <td>...</td>\n",
       "      <td>18.039638</td>\n",
       "      <td>13.989973</td>\n",
       "      <td>0.699499</td>\n",
       "      <td>18.152227</td>\n",
       "      <td>8.948391</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7.158713</td>\n",
       "      <td>-10.993514</td>\n",
       "      <td>2.535683</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACX CRYSTAL</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>Container</td>\n",
       "      <td>39565</td>\n",
       "      <td>39565</td>\n",
       "      <td>28880</td>\n",
       "      <td>1</td>\n",
       "      <td>28880</td>\n",
       "      <td>21660.00</td>\n",
       "      <td>...</td>\n",
       "      <td>20.608973</td>\n",
       "      <td>14.383796</td>\n",
       "      <td>0.719190</td>\n",
       "      <td>17.666437</td>\n",
       "      <td>6.957362</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.565890</td>\n",
       "      <td>-12.100548</td>\n",
       "      <td>3.174055</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACX DIAMOND</td>\n",
       "      <td>SINGAPORE</td>\n",
       "      <td>Container</td>\n",
       "      <td>39598</td>\n",
       "      <td>39598</td>\n",
       "      <td>28880</td>\n",
       "      <td>1</td>\n",
       "      <td>28880</td>\n",
       "      <td>21660.00</td>\n",
       "      <td>...</td>\n",
       "      <td>20.606700</td>\n",
       "      <td>14.384442</td>\n",
       "      <td>0.719222</td>\n",
       "      <td>17.653661</td>\n",
       "      <td>6.954532</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.563625</td>\n",
       "      <td>-12.090036</td>\n",
       "      <td>3.173050</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Status    Nama Kapal Flag State          Jenis Kapal    DWT  Capacity  \\\n",
       "0  ACTIVE        ACACIA     PANAMA  Oil/Chemical Tanker  13565     13565   \n",
       "1  ACTIVE   ACADIA PARK    LIBERIA  Oil/Chemical Tanker  19996     19996   \n",
       "2  ACTIVE  ABRAO COCHIN     PANAMA            Container  23623     23623   \n",
       "3  ACTIVE   ACX CRYSTAL     PANAMA            Container  39565     39565   \n",
       "4  ACTIVE   ACX DIAMOND  SINGAPORE            Container  39598     39598   \n",
       "\n",
       "   EHP ME  No ME  MCR ME       PME  ...       Vapp       Vavg    Margin  \\\n",
       "0    4900      1    4900   3675.00  ...  13.144406  13.578394  0.678920   \n",
       "1    4900      1    4900   3675.00  ...  12.486842  13.865005  0.693250   \n",
       "2   15785      1   15785  11838.75  ...  18.039638  13.989973  0.699499   \n",
       "3   28880      1   28880  21660.00  ...  20.608973  14.383796  0.719190   \n",
       "4   28880      1   28880  21660.00  ...  20.606700  14.384442  0.719222   \n",
       "\n",
       "   Att. EEXI  Reference Line  Reduction Factor  Req. EEXI  Req - Att  \\\n",
       "0  12.359438       11.730367          0.119563  10.327855  -2.031583   \n",
       "1   9.693722        9.706713          0.199950   7.765856  -1.927866   \n",
       "2  18.152227        8.948391          0.200000   7.158713 -10.993514   \n",
       "3  17.666437        6.957362          0.200000   5.565890 -12.100548   \n",
       "4  17.653661        6.954532          0.200000   5.563625 -12.090036   \n",
       "\n",
       "    Att/Req    Judgment  \n",
       "0  1.196709  Not Comply  \n",
       "1  1.248249  Not Comply  \n",
       "2  2.535683  Not Comply  \n",
       "3  3.174055  Not Comply  \n",
       "4  3.173050  Not Comply  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdata = pd.read_csv(\"Excel Perhitungan EEXI_Container Ship Trial Data.csv\")\n",
    "inputdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features :\n",
      " [[1.35650e+04 1.00000e+01]\n",
      " [1.99960e+04 5.30000e+00]\n",
      " [2.36230e+04 2.30000e+01]\n",
      " [3.95650e+04 2.53000e+01]\n",
      " [3.95980e+04 2.48000e+01]\n",
      " [3.95800e+04 2.57000e+01]\n",
      " [6.69400e+04 2.88000e+01]\n",
      " [5.13140e+04 2.64000e+01]\n",
      " [1.77320e+04 2.00000e+01]\n",
      " [2.19720e+04 2.15000e+01]\n",
      " [9.07990e+04 2.90000e+01]\n",
      " [1.17930e+04 1.74000e+01]\n",
      " [9.06470e+04 2.91000e+01]\n",
      " [7.24470e+04 2.88000e+01]\n",
      " [7.24470e+04 2.89000e+01]\n",
      " [7.29120e+04 2.89000e+01]\n",
      " [7.29120e+04 2.86000e+01]\n",
      " [6.69400e+04 2.88000e+01]\n",
      " [6.15300e+03 9.90000e+00]\n",
      " [8.11710e+04 2.71000e+01]\n",
      " [3.33040e+04 2.45000e+01]\n",
      " [8.11710e+04 2.73000e+01]\n",
      " [2.47270e+04 1.65000e+01]\n",
      " [2.92660e+04 2.27000e+01]\n",
      " [5.13140e+04 2.65000e+01]\n",
      " [5.24520e+04 2.62000e+01]\n",
      " [2.47090e+04 1.88000e+01]\n",
      " [5.21180e+04 2.74000e+01]\n",
      " [5.21180e+04 2.83000e+01]\n",
      " [5.21180e+04 2.72000e+01]\n",
      " [2.19730e+04 2.17000e+01]\n",
      " [1.12020e+04 1.28000e+01]\n",
      " [1.20830e+04 1.31000e+01]\n",
      " [1.11700e+04 1.44000e+01]\n",
      " [7.20200e+03 1.54000e+01]\n",
      " [5.15330e+04 1.98000e+01]\n",
      " [7.29820e+04 2.91000e+01]\n",
      " [1.11700e+04 1.25000e+01]\n",
      " [1.19680e+04 1.62000e+01]\n",
      " [6.83200e+03 1.14000e+01]\n",
      " [3.92660e+04 1.78000e+01]\n",
      " [1.19830e+04 1.48000e+01]\n",
      " [7.28070e+04 2.88000e+01]\n",
      " [7.29680e+04 2.81000e+01]\n",
      " [6.23100e+03 1.35000e+01]\n",
      " [2.47690e+04 1.80000e+01]\n",
      " [3.29710e+04 1.68000e+01]\n",
      " [6.69400e+04 2.87000e+01]\n",
      " [2.84510e+04 1.32000e+01]\n",
      " [1.78520e+04 2.08000e+01]\n",
      " [3.57290e+04 1.65000e+01]\n",
      " [5.20550e+04 2.73000e+01]\n",
      " [1.16560e+04 1.35000e+01]\n",
      " [2.14420e+04 2.17000e+01]\n",
      " [3.34430e+04 2.37000e+01]\n",
      " [2.14230e+04 2.14000e+01]\n",
      " [8.05470e+04 2.28000e+01]\n",
      " [1.31095e+05 1.75000e+01]\n",
      " [1.31095e+05 1.65000e+01]\n",
      " [1.31095e+05 1.79000e+01]\n",
      " [1.31095e+05 1.72000e+01]\n",
      " [8.05510e+04 2.26000e+01]\n",
      " [2.47730e+04 1.81000e+01]\n",
      " [1.16851e+05 2.25000e+01]\n",
      " [1.18170e+04 1.38000e+01]\n",
      " [3.88970e+04 1.91000e+01]\n",
      " [3.88830e+04 1.51000e+01]\n",
      " [2.20810e+04 1.45000e+01]\n",
      " [3.89040e+04 1.99000e+01]\n",
      " [2.20810e+04 1.52000e+01]\n",
      " [2.20810e+04 1.63000e+01]\n",
      " [3.89210e+04 1.64000e+01]\n",
      " [3.89380e+04 1.73000e+01]\n",
      " [2.20810e+04 1.55000e+01]\n",
      " [1.36840e+04 1.81000e+01]\n",
      " [1.37010e+04 1.66000e+01]\n",
      " [1.37070e+04 1.76000e+01]\n",
      " [1.36840e+04 1.69000e+01]\n",
      " [1.36730e+04 1.99000e+01]\n",
      " [1.36860e+04 1.39000e+01]\n",
      " [1.37090e+04 1.08000e+01]\n",
      " [1.36870e+04 1.82000e+01]\n",
      " [1.37060e+04 1.74000e+01]\n",
      " [3.71150e+04 2.42000e+01]\n",
      " [2.19350e+04 2.15000e+01]\n",
      " [1.16851e+05 1.76000e+01]\n",
      " [1.31770e+04 1.52000e+01]\n",
      " [1.56390e+04 1.96000e+01]\n",
      " [1.72240e+04 2.02000e+01]\n",
      " [3.93570e+04 1.76000e+01]\n",
      " [6.59500e+04 2.76000e+01]\n",
      " [7.44530e+04 2.88000e+01]\n",
      " [8.85300e+03 1.70000e+01]\n",
      " [6.83200e+03 1.65000e+01]\n",
      " [6.27300e+03 1.48000e+01]\n",
      " [2.49240e+04 2.19000e+01]\n",
      " [1.77040e+04 1.99000e+01]\n",
      " [2.49730e+04 2.23000e+01]\n",
      " [1.77050e+04 2.03000e+01]\n",
      " [2.33730e+04 1.74000e+01]\n",
      " [7.13100e+03 1.48000e+01]\n",
      " [2.58640e+04 1.80000e+01]\n",
      " [1.18170e+04 1.56000e+01]\n",
      " [1.17860e+04 2.00000e+01]\n",
      " [2.41960e+05 1.89000e+01]\n",
      " [2.41960e+05 2.17000e+01]\n",
      " [2.41960e+05 1.69000e+01]\n",
      " [2.41960e+05 1.60000e+01]\n",
      " [3.73010e+04 1.74000e+01]\n",
      " [3.73010e+04 2.09000e+01]\n",
      " [3.73010e+04 1.98000e+01]\n",
      " [3.73010e+04 1.61000e+01]\n",
      " [3.69720e+04 2.04000e+01]\n",
      " [3.69720e+04 1.63000e+01]\n",
      " [3.69720e+04 2.03000e+01]\n",
      " [3.69720e+04 2.02000e+01]\n",
      " [3.73010e+04 1.93000e+01]\n",
      " [3.69720e+04 1.95000e+01]\n",
      " [2.22170e+04 1.62000e+01]\n",
      " [2.23830e+04 1.75000e+01]\n",
      " [2.21490e+04 1.20000e+01]\n",
      " [2.23980e+04 1.68000e+01]\n",
      " [2.20530e+04 1.90000e+01]\n",
      " [2.23900e+04 1.69000e+01]\n",
      " [2.23820e+04 1.32000e+01]\n",
      " [2.20500e+04 1.91000e+01]\n",
      " [2.24050e+04 1.75000e+01]\n",
      " [2.23830e+04 1.57000e+01]\n",
      " [2.23890e+04 1.56000e+01]\n",
      " [2.23770e+04 1.64000e+01]\n",
      " [2.23770e+04 1.84000e+01]\n",
      " [2.23770e+04 1.80000e+01]\n",
      " [2.24000e+04 1.69000e+01]\n",
      " [2.21490e+04 1.70000e+01]\n",
      " [2.22170e+04 1.97000e+01]\n",
      " [2.22170e+04 1.80000e+01]\n",
      " [2.21490e+04 1.59000e+01]\n",
      " [2.21490e+04 1.31000e+01]\n",
      " [2.23630e+04 1.95000e+01]\n",
      " [1.30573e+05 2.20000e+01]\n",
      " [1.30573e+05 2.35000e+01]\n",
      " [1.30573e+05 2.31000e+01]\n",
      " [1.30573e+05 1.59000e+01]\n",
      " [1.30573e+05 2.37000e+01]\n",
      " [1.30573e+05 1.74000e+01]\n",
      " [1.98886e+05 1.35000e+01]\n",
      " [1.98937e+05 1.77000e+01]\n",
      " [1.98886e+05 1.21000e+01]\n",
      " [1.98937e+05 1.60000e+01]\n",
      " [1.03668e+05 2.64000e+01]\n",
      " [1.03518e+05 2.60000e+01]\n",
      " [1.03723e+05 2.59000e+01]\n",
      " [1.03668e+05 2.61000e+01]\n",
      " [1.03891e+05 2.65000e+01]\n",
      " [1.03668e+05 2.58000e+01]\n",
      " [1.03891e+05 2.70000e+01]\n",
      " [1.03891e+05 2.71000e+01]\n",
      " [1.56238e+05 2.19000e+01]\n",
      " [1.56174e+05 2.24000e+01]\n",
      " [3.29630e+04 1.55000e+01]\n",
      " [3.35410e+04 1.88000e+01]\n",
      " [3.35410e+04 1.81000e+01]\n",
      " [3.28310e+04 1.85000e+01]\n",
      " [3.35010e+04 1.74000e+01]\n",
      " [3.35770e+04 1.82000e+01]\n",
      " [3.28120e+04 2.09000e+01]\n",
      " [3.29260e+04 1.67000e+01]\n",
      " [3.35410e+04 1.89000e+01]\n",
      " [3.35440e+04 1.83000e+01]\n",
      " [3.34960e+04 1.69000e+01]\n",
      " [1.93090e+04 2.15000e+01]\n",
      " [1.93090e+04 2.14000e+01]\n",
      " [1.93110e+04 2.12000e+01]\n",
      " [7.82680e+04 1.72000e+01]\n",
      " [7.82680e+04 1.82000e+01]\n",
      " [1.52344e+05 1.81000e+01]\n",
      " [6.32160e+04 1.79000e+01]\n",
      " [6.32160e+04 1.85000e+01]\n",
      " [6.32160e+04 1.74000e+01]\n",
      " [6.32160e+04 2.02000e+01]\n",
      " [6.32160e+04 2.04000e+01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_capacity_array = np.array(inputdata['Capacity'])\n",
    "feature_speed_array = np.array(inputdata['Vs'])\n",
    "\n",
    "features = np.stack((feature_capacity_array,feature_speed_array),axis=1)\n",
    "print('features :\\n',features)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target PME :\n",
      " [ 3675.    3675.   11838.75 21660.   21660.   21660.   42900.   27120.\n",
      "  7220.25 11865.   48075.    5175.   48075.   47190.   47190.   47190.\n",
      " 47190.   42900.    1631.25 46147.5  16301.25 46012.5   7785.   13547.25\n",
      " 27120.   25290.    7785.   30510.   30510.   30510.   11865.    6300.\n",
      "  7200.    6300.    3975.   27120.   47190.    6300.    7200.    1575.\n",
      " 18952.5   7200.   47190.   47190.    3150.    7785.   18952.5  42900.\n",
      "  9000.    9480.   10650.   30510.    4440.   11865.   16293.75 11865.\n",
      " 20295.   30502.5  30502.5  30502.5  30502.5  20295.    7785.   31110.\n",
      "  5175.   12690.   12690.   10050.   12690.   10050.   10050.   12690.\n",
      " 12690.   10050.    5212.5   5212.5   5212.5   5212.5   5212.5   5212.5\n",
      "  5212.5   5212.5   5212.5  16301.25 11865.   31110.    3330.    6420.75\n",
      "  7220.25 18952.5  38542.5  45292.5   3353.25  2514.75  2520.   10314.75\n",
      "  7220.25 10314.75  7220.25 12495.    3975.   12495.    5475.    5475.\n",
      " 43950.   43950.   43950.   43950.   18195.   18195.   18195.   18195.\n",
      " 18195.   18195.   18195.   18195.   18195.   18195.   10050.    8790.\n",
      "  8970.    8970.    8925.    8970.    8970.    8925.    8970.    8790.\n",
      "  8790.    8790.    8790.    8790.    8790.    8970.   10050.   10050.\n",
      "  8970.    8970.    8790.   30810.   30810.   30810.   30810.   30810.\n",
      " 30810.   44475.   44475.   44475.   44475.   42052.5  42052.5  42052.5\n",
      " 42052.5  42052.5  42052.5  42052.5  42052.5  36375.   36375.   11250.\n",
      " 10612.5  10612.5  11250.   10612.5  10612.5  11250.   11250.   10612.5\n",
      " 10612.5  10612.5   9360.    9360.    9360.   41175.   41175.   39937.5\n",
      " 36450.   36450.   36450.   36450.   36450.  ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_pme = np.array(inputdata['PME'])\n",
    "print('target PME :\\n',target_pme)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target PAE :\n",
      " [  43.8903   367.5     1091.9375  2374.5     2374.5     2374.5\n",
      " 3967.5     2784.      1083.0375  1639.875   4355.625    776.25\n",
      " 4355.625   4289.25    4289.25    4289.25    4289.25    3967.5\n",
      "  244.6875  4211.0625  1972.59375 4200.9375  1167.75    1766.04375\n",
      " 2784.      1764.5     1167.75    3038.25    3038.25    3038.25\n",
      " 1639.875    945.      1080.       945.       596.25    2784.\n",
      " 4289.25     945.      1080.       236.25    2171.4375  1080.\n",
      " 4289.25    4289.25     472.5     1167.75    2171.4375  3967.5\n",
      " 1350.      1422.      1548.75    3038.25     666.      1639.875\n",
      " 1972.03125 1639.875   2272.125   3037.6875  3037.6875  3037.6875\n",
      " 3037.6875  2272.125   1167.75    3083.25     776.25    1701.75\n",
      " 1701.75    1503.75    1701.75    1503.75    1503.75    1701.75\n",
      " 1701.75    1503.75     781.875    781.875    781.875    781.875\n",
      "  781.875    781.875    781.875    781.875    781.875   1972.59375\n",
      " 1639.875   3083.25     499.5      963.1125  1083.0375  2171.4375\n",
      " 3640.6875  4146.9375   502.9875   377.2125   378.      1523.60625\n",
      " 1083.0375  1523.60625 1083.0375  1687.125    596.25    1687.125\n",
      "  821.25     821.25    4046.25    4046.25    4046.25    4046.25\n",
      " 2114.625   2114.625   2114.625   2114.625   2114.625   2114.625\n",
      " 2114.625   2114.625   2114.625   2114.625   1503.75    1318.5\n",
      " 1345.5     1345.5     1338.75    1345.5     1345.5     1338.75\n",
      " 1345.5     1318.5     1318.5     1318.5     1318.5     1318.5\n",
      " 1318.5     1345.5     1503.75    1503.75    1345.5     1345.5\n",
      " 1318.5     3060.75    3060.75    3060.75    3060.75    3060.75\n",
      " 3060.75    4085.625   4085.625   4085.625   4085.625   3903.9375\n",
      " 3903.9375  3903.9375  3903.9375  3903.9375  3903.9375  3903.9375\n",
      " 3903.9375  3478.125   3478.125   1593.75    1545.9375  1545.9375\n",
      " 1593.75    1545.9375  1545.9375  1593.75    1593.75    1545.9375\n",
      " 1545.9375  1545.9375  1404.      1404.      1404.      3838.125\n",
      " 3838.125   3745.3125  3483.75    3483.75    3483.75    3483.75\n",
      " 3483.75   ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_pae = np.array(inputdata['PAE'])\n",
    "print('target PAE :\\n',target_pae)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target Vapp :\n",
      " [13.14440569 12.48684212 18.03963759 20.60897256 20.60670023 20.60793942\n",
      " 24.14266635 21.46174552 15.88983737 18.2267802  24.08587329 15.00833684\n",
      " 24.091211   24.66276708 24.66276708 24.6419069  24.6419069  24.14266635\n",
      " 11.13192003 24.11442826 19.17808527 24.09089049 15.59272086 18.34168758\n",
      " 21.46174552 20.90699046 15.59422269 22.27526614 22.27526614 22.27526614\n",
      " 18.22667049 16.13475679 16.70108084 16.14086252 14.67124809 21.44966061\n",
      " 24.6387797  16.14086252 16.72221742 10.8511952  19.73155955 16.71944747\n",
      " 24.64660407 24.63940487 13.83916433 15.58922141 20.1928514  24.14266635\n",
      " 16.06418331 17.38404161 16.48710542 22.27882976 14.28327982 18.285736\n",
      " 19.16458395 18.28788006 18.35694994 19.71539596 19.71539596 19.71539596\n",
      " 19.71539596 18.35682938 15.58888848 20.1496298  15.0043019  17.28364924\n",
      " 17.28447215 17.23422758 17.28323791 17.23422758 17.23422758 17.28223931\n",
      " 17.28124121 17.23422758 14.75147716 14.74905511 14.74820108 14.75147716\n",
      " 14.75304619 14.75119204 14.74791649 14.75104949 14.74834339 18.90523862\n",
      " 18.23084345 20.1496298  12.7684089  15.53620927 15.95104064 19.72551959\n",
      " 23.34184869 24.23995291 13.48926204 12.68281514 12.83573271 17.10800898\n",
      " 15.89315881 17.10356561 15.89304008 18.39285086 14.6904845  18.1481449\n",
      " 15.28881118 15.29412359 20.53425203 20.53425203 20.53425203 20.53425203\n",
      " 19.59771406 19.59771406 19.59771406 19.59771406 19.62069011 19.62069011\n",
      " 19.62069011 19.62069011 19.59771406 19.62069011 17.22023753 16.45201786\n",
      " 16.58659869 16.56209287 16.56833089 16.5628754  16.56365824 16.568629\n",
      " 16.56140841 16.45201786 16.45143468 16.45260122 16.45260122 16.45260122\n",
      " 16.45036597 16.58659869 17.22023753 17.22023753 16.58659869 16.58659869\n",
      " 16.45396308 19.79186675 19.79186675 19.79186675 19.79186675 19.79186675\n",
      " 19.79186675 21.15719857 21.15648115 21.15719857 21.15648115 22.6346298\n",
      " 22.63896485 22.63304207 22.6346298  22.62819815 22.6346298  22.62819815\n",
      " 22.62819815 20.4275679  20.42867485 16.9709802  16.60594308 16.60594308\n",
      " 16.97998881 16.60856403 16.60358725 16.98128888 16.97350121 16.60594308\n",
      " 16.60574665 16.60889189 17.13169055 17.13169055 17.13145588 23.32727297\n",
      " 23.32727297 21.14425208 23.04020687 23.04020687 23.04020687 23.04020687\n",
      " 23.04020687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_vapp = np.array(inputdata['Vapp'])\n",
    "print('target Vapp :\\n',target_vapp)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Train and Test Data using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For main engine\n",
    "features_train_m,features_test_m,y_train_list_m,y_test_list_m = train_test_split(features,target_pme,test_size=0.25,random_state=20)\n",
    "\n",
    "\n",
    "# For aux engine\n",
    "features_train_a,features_test_a,y_train_list_a,y_test_list_a = train_test_split(features,target_pae,test_size=0.25,random_state=20)\n",
    "\n",
    "# For Vapp\n",
    "features_train_v,features_test_v,y_train_list_v,y_test_list_v = train_test_split(features,target_vapp,test_size=0.25,random_state=20)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train_m)\n",
    "\n",
    "features_train_m_scaled = scaler.transform(features_train_m)\n",
    "features_test_m_scaled = scaler.transform(features_test_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Artificial Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_28 (Dense)            (None, 128)               384       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513 (2.00 KB)\n",
      "Trainable params: 513 (2.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 54ms/step - loss: 417558912.0000 - mae: 16270.8975 - val_loss: 449103200.0000 - val_mae: 17615.3867\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 310380992.0000 - mae: 13869.0518 - val_loss: 321174752.0000 - val_mae: 14466.3936\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 225555968.0000 - mae: 11546.8262 - val_loss: 231089424.0000 - val_mae: 11438.5068\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 158874560.0000 - mae: 9197.6318 - val_loss: 177450608.0000 - val_mae: 9492.7334\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 121399536.0000 - mae: 7727.8843 - val_loss: 152148368.0000 - val_mae: 8938.2637\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 101550008.0000 - mae: 7003.2603 - val_loss: 148338576.0000 - val_mae: 8768.2773\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91182352.0000 - mae: 6587.2769 - val_loss: 154628640.0000 - val_mae: 8688.9131\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 89061032.0000 - mae: 6363.2310 - val_loss: 162083264.0000 - val_mae: 8730.5234\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91074376.0000 - mae: 6285.4087 - val_loss: 168456176.0000 - val_mae: 8771.1445\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 93010744.0000 - mae: 6244.7720 - val_loss: 171614848.0000 - val_mae: 8796.2910\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 93909456.0000 - mae: 6247.8101 - val_loss: 170233920.0000 - val_mae: 8785.4102\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 93505064.0000 - mae: 6243.2729 - val_loss: 169795984.0000 - val_mae: 8781.8359\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 93460992.0000 - mae: 6271.9282 - val_loss: 166437888.0000 - val_mae: 8754.9453\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 92699104.0000 - mae: 6299.0010 - val_loss: 163692432.0000 - val_mae: 8739.5449\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91397192.0000 - mae: 6303.8525 - val_loss: 161651136.0000 - val_mae: 8727.1865\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90998560.0000 - mae: 6338.1934 - val_loss: 157905552.0000 - val_mae: 8701.9688\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90901624.0000 - mae: 6408.4971 - val_loss: 155331472.0000 - val_mae: 8689.2773\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90136480.0000 - mae: 6425.0669 - val_loss: 154241344.0000 - val_mae: 8686.6836\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90109664.0000 - mae: 6446.9160 - val_loss: 153778624.0000 - val_mae: 8685.4404\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90171304.0000 - mae: 6458.7949 - val_loss: 153602368.0000 - val_mae: 8684.8682\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90508888.0000 - mae: 6456.4775 - val_loss: 154361280.0000 - val_mae: 8686.5459\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 90182712.0000 - mae: 6442.8813 - val_loss: 153390528.0000 - val_mae: 8684.0527\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90293008.0000 - mae: 6470.0117 - val_loss: 152640816.0000 - val_mae: 8681.9521\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90396232.0000 - mae: 6503.7729 - val_loss: 151298432.0000 - val_mae: 8693.2725\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90634232.0000 - mae: 6545.1987 - val_loss: 150769280.0000 - val_mae: 8701.9707\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90498664.0000 - mae: 6543.8066 - val_loss: 151722912.0000 - val_mae: 8685.9727\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90206704.0000 - mae: 6491.7573 - val_loss: 153604672.0000 - val_mae: 8683.8184\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90202720.0000 - mae: 6461.9614 - val_loss: 154286256.0000 - val_mae: 8685.2920\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90915056.0000 - mae: 6431.7847 - val_loss: 156413008.0000 - val_mae: 8689.7393\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 90232224.0000 - mae: 6411.5688 - val_loss: 154600832.0000 - val_mae: 8685.6885\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 90176512.0000 - mae: 6435.3125 - val_loss: 154323568.0000 - val_mae: 8684.8887\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90446384.0000 - mae: 6435.5059 - val_loss: 154817472.0000 - val_mae: 8685.8564\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90090392.0000 - mae: 6431.5981 - val_loss: 153948656.0000 - val_mae: 8683.6855\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 90064480.0000 - mae: 6449.0410 - val_loss: 153084592.0000 - val_mae: 8681.3857\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90260768.0000 - mae: 6476.3364 - val_loss: 152723072.0000 - val_mae: 8680.2686\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90345064.0000 - mae: 6470.8960 - val_loss: 153845808.0000 - val_mae: 8682.9414\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90494408.0000 - mae: 6444.6562 - val_loss: 155113696.0000 - val_mae: 8685.6816\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90142528.0000 - mae: 6419.2363 - val_loss: 155319440.0000 - val_mae: 8685.9697\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90122600.0000 - mae: 6409.9722 - val_loss: 156011792.0000 - val_mae: 8687.2637\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90135360.0000 - mae: 6397.9346 - val_loss: 156611360.0000 - val_mae: 8689.3516\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90639680.0000 - mae: 6387.5962 - val_loss: 157906288.0000 - val_mae: 8698.9111\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90538560.0000 - mae: 6379.6348 - val_loss: 157325296.0000 - val_mae: 8694.5166\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90391736.0000 - mae: 6379.0972 - val_loss: 157721616.0000 - val_mae: 8697.3008\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 91466488.0000 - mae: 6380.4414 - val_loss: 158869136.0000 - val_mae: 8705.3125\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90383752.0000 - mae: 6395.8872 - val_loss: 154496528.0000 - val_mae: 8682.9512\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90402240.0000 - mae: 6447.9619 - val_loss: 152169920.0000 - val_mae: 8678.4688\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90232608.0000 - mae: 6501.4663 - val_loss: 151468464.0000 - val_mae: 8683.5693\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90374616.0000 - mae: 6521.4971 - val_loss: 151756880.0000 - val_mae: 8679.7637\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90417336.0000 - mae: 6489.9966 - val_loss: 153280144.0000 - val_mae: 8679.3145\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90143576.0000 - mae: 6460.7095 - val_loss: 153168224.0000 - val_mae: 8678.8516\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90056192.0000 - mae: 6471.7246 - val_loss: 152042112.0000 - val_mae: 8677.8604\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90572480.0000 - mae: 6527.4341 - val_loss: 150619152.0000 - val_mae: 8696.0830\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90595104.0000 - mae: 6555.5566 - val_loss: 150643344.0000 - val_mae: 8695.3115\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90317760.0000 - mae: 6537.6919 - val_loss: 151493344.0000 - val_mae: 8680.8965\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 90064728.0000 - mae: 6496.4961 - val_loss: 152855536.0000 - val_mae: 8677.1641\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 91093440.0000 - mae: 6468.6147 - val_loss: 155048432.0000 - val_mae: 8682.1963\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90200440.0000 - mae: 6430.3818 - val_loss: 153056896.0000 - val_mae: 8677.3252\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90077088.0000 - mae: 6467.0522 - val_loss: 152589616.0000 - val_mae: 8675.9209\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90246904.0000 - mae: 6476.5522 - val_loss: 152405808.0000 - val_mae: 8675.2266\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 89924656.0000 - mae: 6483.9805 - val_loss: 151078960.0000 - val_mae: 8685.4893\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90355360.0000 - mae: 6538.1113 - val_loss: 150030224.0000 - val_mae: 8703.8848\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90823008.0000 - mae: 6583.1387 - val_loss: 149525904.0000 - val_mae: 8713.9775\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90827824.0000 - mae: 6581.3286 - val_loss: 150715216.0000 - val_mae: 8690.5361\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90232680.0000 - mae: 6520.6025 - val_loss: 152412192.0000 - val_mae: 8674.2803\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90365992.0000 - mae: 6460.9824 - val_loss: 154078096.0000 - val_mae: 8678.2637\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90316680.0000 - mae: 6434.3022 - val_loss: 153967632.0000 - val_mae: 8677.8115\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90243032.0000 - mae: 6459.0996 - val_loss: 152201520.0000 - val_mae: 8673.3623\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90097320.0000 - mae: 6483.0752 - val_loss: 152535776.0000 - val_mae: 8673.8486\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 89913392.0000 - mae: 6458.1875 - val_loss: 154565328.0000 - val_mae: 8678.5996\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89795992.0000 - mae: 6407.7661 - val_loss: 156605824.0000 - val_mae: 8685.0195\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90626712.0000 - mae: 6376.3877 - val_loss: 159125888.0000 - val_mae: 8703.0332\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91809336.0000 - mae: 6358.1665 - val_loss: 161355600.0000 - val_mae: 8717.3525\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90536952.0000 - mae: 6343.1074 - val_loss: 156802768.0000 - val_mae: 8686.0879\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89966240.0000 - mae: 6385.4897 - val_loss: 154507936.0000 - val_mae: 8677.5293\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90095248.0000 - mae: 6434.5708 - val_loss: 153446592.0000 - val_mae: 8674.8633\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90041344.0000 - mae: 6438.9751 - val_loss: 154205696.0000 - val_mae: 8676.4736\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90028792.0000 - mae: 6416.2969 - val_loss: 155201056.0000 - val_mae: 8678.4990\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 90236000.0000 - mae: 6402.6367 - val_loss: 155917904.0000 - val_mae: 8679.8076\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90137744.0000 - mae: 6407.9722 - val_loss: 154315744.0000 - val_mae: 8676.1543\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90189656.0000 - mae: 6422.1938 - val_loss: 154586496.0000 - val_mae: 8676.5684\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89958824.0000 - mae: 6419.3379 - val_loss: 153266176.0000 - val_mae: 8673.2773\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89560688.0000 - mae: 6457.4473 - val_loss: 150789680.0000 - val_mae: 8682.5498\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90373680.0000 - mae: 6548.6465 - val_loss: 149739648.0000 - val_mae: 8701.5508\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90974120.0000 - mae: 6574.1533 - val_loss: 149859168.0000 - val_mae: 8698.7578\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90699456.0000 - mae: 6584.2456 - val_loss: 148844656.0000 - val_mae: 8721.2363\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91032784.0000 - mae: 6621.1499 - val_loss: 149130208.0000 - val_mae: 8713.7041\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90845184.0000 - mae: 6584.1899 - val_loss: 150643600.0000 - val_mae: 8683.1670\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90240352.0000 - mae: 6506.8706 - val_loss: 153072032.0000 - val_mae: 8671.3984\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89741240.0000 - mae: 6431.2915 - val_loss: 155267440.0000 - val_mae: 8676.2490\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89848632.0000 - mae: 6388.1377 - val_loss: 157086080.0000 - val_mae: 8685.6484\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90153984.0000 - mae: 6356.0278 - val_loss: 158691520.0000 - val_mae: 8696.9756\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 91187600.0000 - mae: 6350.9023 - val_loss: 159041168.0000 - val_mae: 8699.2070\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90007584.0000 - mae: 6361.2095 - val_loss: 154426032.0000 - val_mae: 8673.6260\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 89690720.0000 - mae: 6426.4619 - val_loss: 152169488.0000 - val_mae: 8667.8760\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90288880.0000 - mae: 6492.4863 - val_loss: 151046928.0000 - val_mae: 8673.5732\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90166688.0000 - mae: 6504.9072 - val_loss: 151407120.0000 - val_mae: 8669.5518\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90204680.0000 - mae: 6484.5322 - val_loss: 152691056.0000 - val_mae: 8668.6602\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 89899528.0000 - mae: 6446.7070 - val_loss: 154265840.0000 - val_mae: 8672.2607\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90336056.0000 - mae: 6406.2856 - val_loss: 156434736.0000 - val_mae: 8679.3369\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 89994992.0000 - mae: 6375.8867 - val_loss: 155818048.0000 - val_mae: 8675.2002\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#for main engine\n",
    "# Using dense layer\n",
    "ANNmodel_m = Sequential()\n",
    "ANNmodel_m.add(Dense(128, input_dim=2, activation='relu'))\n",
    "#Output layer\n",
    "ANNmodel_m.add(Dense(1, activation='linear'))\n",
    "\n",
    "ANNmodel_m.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "ANNmodel_m.summary()\n",
    "\n",
    "history = ANNmodel_m.fit(features_train_m, y_train_list_m, validation_split=0.2, epochs =100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 128)               384       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513 (2.00 KB)\n",
      "Trainable params: 513 (2.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 54ms/step - loss: 1399402.3750 - mae: 853.8668 - val_loss: 2572666.2500 - val_mae: 1011.9136\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1411732.1250 - mae: 937.6394 - val_loss: 1851563.5000 - val_mae: 1153.1205\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1412947.1250 - mae: 1009.5536 - val_loss: 1556558.5000 - val_mae: 1008.4786\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1163488.2500 - mae: 814.3500 - val_loss: 2112272.2500 - val_mae: 992.1171\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1174878.8750 - mae: 833.8862 - val_loss: 1484097.8750 - val_mae: 1024.6790\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1099755.0000 - mae: 903.1299 - val_loss: 1505065.8750 - val_mae: 1015.0363\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1014313.0625 - mae: 863.0505 - val_loss: 1784415.7500 - val_mae: 994.7151\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1297746.1250 - mae: 801.6193 - val_loss: 1978028.5000 - val_mae: 992.1636\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1073025.1250 - mae: 808.9664 - val_loss: 1501324.5000 - val_mae: 1041.8696\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1173186.5000 - mae: 930.3480 - val_loss: 1480956.0000 - val_mae: 1022.4235\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1061883.8750 - mae: 878.1722 - val_loss: 1712098.1250 - val_mae: 994.7559\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1071887.5000 - mae: 800.6504 - val_loss: 1720772.1250 - val_mae: 994.3655\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1078220.8750 - mae: 858.6984 - val_loss: 1481359.2500 - val_mae: 1028.6981\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1079722.6250 - mae: 895.6165 - val_loss: 1564613.1250 - val_mae: 1003.8609\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1042035.2500 - mae: 817.7214 - val_loss: 1921863.8750 - val_mae: 990.9755\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1158241.3750 - mae: 805.6271 - val_loss: 1658301.0000 - val_mae: 995.2202\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1062688.1250 - mae: 840.8704 - val_loss: 1519743.1250 - val_mae: 1008.2469\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1033943.0625 - mae: 859.8185 - val_loss: 1586310.2500 - val_mae: 1000.3738\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1023595.5000 - mae: 821.4121 - val_loss: 1811033.1250 - val_mae: 991.2739\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1148802.6250 - mae: 811.8394 - val_loss: 1594128.2500 - val_mae: 998.9969\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1079511.8750 - mae: 871.3257 - val_loss: 1497003.2500 - val_mae: 1010.5406\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 994833.0625 - mae: 847.2122 - val_loss: 1797166.6250 - val_mae: 990.6814\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1113904.7500 - mae: 780.5687 - val_loss: 1748364.8750 - val_mae: 991.0890\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1114151.0000 - mae: 870.1241 - val_loss: 1520467.5000 - val_mae: 1048.1804\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1116427.1250 - mae: 909.9125 - val_loss: 1608733.8750 - val_mae: 996.0563\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1018546.0000 - mae: 795.7366 - val_loss: 2129654.2500 - val_mae: 988.4078\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1194524.7500 - mae: 774.0178 - val_loss: 1669913.5000 - val_mae: 991.2307\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1040813.9375 - mae: 842.1571 - val_loss: 1469452.6250 - val_mae: 1016.4462\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1080500.7500 - mae: 895.6021 - val_loss: 1470555.8750 - val_mae: 1014.6639\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1065520.6250 - mae: 861.1026 - val_loss: 1756411.8750 - val_mae: 989.1132\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1044393.1250 - mae: 805.8948 - val_loss: 1618148.7500 - val_mae: 993.2723\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1024611.1250 - mae: 829.2785 - val_loss: 1496337.8750 - val_mae: 1006.1558\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1071240.8750 - mae: 861.3864 - val_loss: 1496461.8750 - val_mae: 1005.7341\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1026792.4375 - mae: 867.8107 - val_loss: 1479677.6250 - val_mae: 1008.7466\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1063495.0000 - mae: 855.9933 - val_loss: 1604342.2500 - val_mae: 993.0131\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1042345.1875 - mae: 853.9529 - val_loss: 1475396.7500 - val_mae: 1008.9164\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1011443.0625 - mae: 856.2298 - val_loss: 1667619.0000 - val_mae: 988.5453\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1033494.5625 - mae: 804.3078 - val_loss: 1819164.2500 - val_mae: 986.1118\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1108337.5000 - mae: 783.2978 - val_loss: 1574176.0000 - val_mae: 994.1964\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1044002.5625 - mae: 867.0810 - val_loss: 1466477.8750 - val_mae: 1024.3336\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1088614.5000 - mae: 891.8035 - val_loss: 1551957.0000 - val_mae: 995.6100\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1049611.7500 - mae: 821.3317 - val_loss: 1645035.2500 - val_mae: 987.5513\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1024620.8750 - mae: 814.7214 - val_loss: 1468830.2500 - val_mae: 1007.3505\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1202102.2500 - mae: 938.9857 - val_loss: 1484701.5000 - val_mae: 1034.6277\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1128057.7500 - mae: 871.5429 - val_loss: 2023999.0000 - val_mae: 981.9219\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1138831.5000 - mae: 774.6098 - val_loss: 1653515.0000 - val_mae: 986.2692\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1009357.5000 - mae: 831.7798 - val_loss: 1458046.8750 - val_mae: 1009.3500\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1050816.8750 - mae: 882.0077 - val_loss: 1463544.6250 - val_mae: 1006.3898\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1008555.6250 - mae: 833.0504 - val_loss: 1796151.5000 - val_mae: 983.3648\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1107413.0000 - mae: 781.3735 - val_loss: 1641423.2500 - val_mae: 985.3636\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1013259.2500 - mae: 838.5045 - val_loss: 1459131.7500 - val_mae: 1006.3464\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1070598.1250 - mae: 887.2574 - val_loss: 1531844.7500 - val_mae: 993.6567\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1137170.1250 - mae: 810.3403 - val_loss: 1845028.1250 - val_mae: 981.6896\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1083898.3750 - mae: 806.8982 - val_loss: 1495189.5000 - val_mae: 997.4490\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1005434.7500 - mae: 849.0020 - val_loss: 1541473.8750 - val_mae: 991.5516\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1119495.0000 - mae: 826.6392 - val_loss: 1707712.1250 - val_mae: 982.6735\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1032854.9375 - mae: 807.9413 - val_loss: 1532565.3750 - val_mae: 991.7484\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1014970.5000 - mae: 850.1235 - val_loss: 1473174.6250 - val_mae: 999.4556\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1038339.6250 - mae: 851.5171 - val_loss: 1506603.0000 - val_mae: 993.9692\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1015001.0625 - mae: 859.8714 - val_loss: 1462077.1250 - val_mae: 1000.9435\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 999514.2500 - mae: 850.3942 - val_loss: 1696675.5000 - val_mae: 981.4319\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1038435.0625 - mae: 786.8010 - val_loss: 1898189.1250 - val_mae: 978.5865\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1068059.3750 - mae: 783.9952 - val_loss: 1498551.8750 - val_mae: 993.4274\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1048829.6250 - mae: 869.7215 - val_loss: 1453917.6250 - val_mae: 1001.1794\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1031083.7500 - mae: 835.6667 - val_loss: 1816964.2500 - val_mae: 978.7000\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1074852.8750 - mae: 772.8560 - val_loss: 1649790.5000 - val_mae: 980.7663\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 979622.8750 - mae: 819.8949 - val_loss: 1441576.0000 - val_mae: 1006.5842\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1038525.6875 - mae: 871.0750 - val_loss: 1509981.3750 - val_mae: 990.1135\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 998392.0625 - mae: 830.4717 - val_loss: 1614672.0000 - val_mae: 980.5329\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 992077.6250 - mae: 817.8226 - val_loss: 1480709.8750 - val_mae: 993.1794\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1010343.0625 - mae: 852.6083 - val_loss: 1473372.5000 - val_mae: 993.9191\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1015549.2500 - mae: 857.5696 - val_loss: 1533184.6250 - val_mae: 986.1282\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1028901.3125 - mae: 809.4011 - val_loss: 1748259.1250 - val_mae: 977.3621\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1076234.2500 - mae: 789.2665 - val_loss: 1568030.2500 - val_mae: 982.2365\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1062259.8750 - mae: 855.0683 - val_loss: 1438020.2500 - val_mae: 1001.3539\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1029485.1250 - mae: 856.5067 - val_loss: 1540068.1250 - val_mae: 984.0164\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 996629.6250 - mae: 819.9562 - val_loss: 1457594.5000 - val_mae: 994.2356\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1029935.2500 - mae: 860.7687 - val_loss: 1433932.1250 - val_mae: 1003.8159\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1093485.5000 - mae: 899.8552 - val_loss: 1440742.3750 - val_mae: 997.5430\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 983950.6875 - mae: 835.8454 - val_loss: 1977916.2500 - val_mae: 972.7432\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1100738.3750 - mae: 751.3292 - val_loss: 1536949.0000 - val_mae: 982.4286\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1148428.1250 - mae: 894.7488 - val_loss: 1533398.1250 - val_mae: 1043.4510\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1043902.0625 - mae: 871.7109 - val_loss: 1973892.7500 - val_mae: 971.9466\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1310073.7500 - mae: 738.4292 - val_loss: 1969089.0000 - val_mae: 971.7270\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 930941.2500 - mae: 796.5587 - val_loss: 1721147.2500 - val_mae: 1106.4447\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1370972.5000 - mae: 995.1386 - val_loss: 1449534.8750 - val_mae: 991.8839\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1019390.6875 - mae: 807.3804 - val_loss: 2128397.7500 - val_mae: 974.8101\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1123858.0000 - mae: 765.6545 - val_loss: 1495953.7500 - val_mae: 984.1650\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 998190.4375 - mae: 842.4363 - val_loss: 1450584.8750 - val_mae: 990.4031\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 990046.0000 - mae: 831.3344 - val_loss: 1671292.1250 - val_mae: 973.7411\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1023362.5625 - mae: 782.0960 - val_loss: 1722579.5000 - val_mae: 972.7327\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1075522.6250 - mae: 780.8569 - val_loss: 1482096.2500 - val_mae: 984.3798\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1072088.6250 - mae: 880.7996 - val_loss: 1432903.1250 - val_mae: 1010.6593\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1006857.3125 - mae: 852.6906 - val_loss: 1772833.8750 - val_mae: 971.2675\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1150674.3750 - mae: 761.8964 - val_loss: 1737065.1250 - val_mae: 971.4689\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1007802.5000 - mae: 827.3161 - val_loss: 1421751.5000 - val_mae: 999.1490\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1059747.7500 - mae: 865.4200 - val_loss: 1443344.0000 - val_mae: 988.4819\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 998180.2500 - mae: 854.9992 - val_loss: 1435860.2500 - val_mae: 989.6799\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1029533.5000 - mae: 821.9410 - val_loss: 1692303.0000 - val_mae: 970.9822\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1000436.0625 - mae: 790.6946 - val_loss: 1525459.0000 - val_mae: 976.7753\n"
     ]
    }
   ],
   "source": [
    "#for auxiliary engine\n",
    "# Using dense layer\n",
    "ANNmodel_a = Sequential()\n",
    "ANNmodel_a.add(Dense(128, input_dim=2, activation='relu'))\n",
    "#Output layer\n",
    "ANNmodel_a.add(Dense(1, activation='linear'))\n",
    "\n",
    "ANNmodel_a.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "ANNmodel_a.summary()\n",
    "\n",
    "history = ANNmodel_a.fit(features_train_a, y_train_list_a, validation_split=0.2, epochs =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 128)               384       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513 (2.00 KB)\n",
      "Trainable params: 513 (2.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 50ms/step - loss: 7501200.5000 - mae: 1827.0906 - val_loss: 30784.6738 - val_mae: 140.2438\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1098043.7500 - mae: 681.7825 - val_loss: 4542525.5000 - val_mae: 1610.0968\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2585288.2500 - mae: 1150.6997 - val_loss: 1309445.5000 - val_mae: 868.5037\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 305714.1250 - mae: 381.4865 - val_loss: 199067.1250 - val_mae: 326.2511\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 454308.9688 - mae: 437.2375 - val_loss: 1027236.4375 - val_mae: 752.6189\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 493860.0312 - mae: 483.8533 - val_loss: 105060.2188 - val_mae: 234.5275\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 31142.7871 - mae: 106.2532 - val_loss: 138317.8438 - val_mae: 288.0118\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 114462.6641 - mae: 250.6136 - val_loss: 116844.6094 - val_mae: 265.3899\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 42428.8516 - mae: 138.2682 - val_loss: 12632.2881 - val_mae: 75.0455\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 24540.1738 - mae: 101.7708 - val_loss: 90557.8438 - val_mae: 217.0880\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 47054.3320 - mae: 140.9330 - val_loss: 6294.6318 - val_mae: 50.3874\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2853.9348 - mae: 34.8163 - val_loss: 17943.3770 - val_mae: 108.8896\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 12006.5869 - mae: 85.9930 - val_loss: 5688.4150 - val_mae: 64.5033\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1843.4302 - mae: 32.2527 - val_loss: 3068.9502 - val_mae: 32.6600\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2958.8489 - mae: 31.2077 - val_loss: 5274.3667 - val_mae: 45.4200\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1677.6439 - mae: 23.5253 - val_loss: 296.3183 - val_mae: 16.6723\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1105.0627 - mae: 29.1906 - val_loss: 2176.1753 - val_mae: 42.2825\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1047.4032 - mae: 28.4979 - val_loss: 399.8240 - val_mae: 13.1331\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 594.4805 - mae: 13.9667 - val_loss: 1754.6694 - val_mae: 23.8347\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 764.5093 - mae: 15.8957 - val_loss: 193.0563 - val_mae: 12.4388\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 284.4002 - mae: 16.2622 - val_loss: 616.4905 - val_mae: 24.3569\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 440.1247 - mae: 20.4871 - val_loss: 185.2548 - val_mae: 12.6862\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 203.9419 - mae: 12.2163 - val_loss: 569.8488 - val_mae: 14.3845\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 330.9156 - mae: 12.6369 - val_loss: 221.0307 - val_mae: 12.3185\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 194.7820 - mae: 13.0770 - val_loss: 262.6683 - val_mae: 15.4996\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 245.1070 - mae: 15.2049 - val_loss: 184.9043 - val_mae: 12.6256\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 184.4594 - mae: 12.3648 - val_loss: 275.9543 - val_mae: 12.5241\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 197.0784 - mae: 12.1084 - val_loss: 198.0807 - val_mae: 12.2754\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 176.9556 - mae: 12.5577 - val_loss: 183.8326 - val_mae: 12.8697\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 181.6646 - mae: 12.9628 - val_loss: 191.6482 - val_mae: 12.3461\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 179.4798 - mae: 12.1815 - val_loss: 231.3263 - val_mae: 12.2943\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 187.6839 - mae: 12.5170 - val_loss: 182.6097 - val_mae: 12.6462\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 176.4304 - mae: 12.6496 - val_loss: 190.8581 - val_mae: 12.3214\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 173.8315 - mae: 12.2029 - val_loss: 206.6731 - val_mae: 12.2089\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 177.6956 - mae: 12.1681 - val_loss: 187.7607 - val_mae: 12.3643\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 174.3556 - mae: 12.5069 - val_loss: 181.3101 - val_mae: 12.6457\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 183.2014 - mae: 12.5687 - val_loss: 199.8583 - val_mae: 12.1776\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 178.6671 - mae: 12.0863 - val_loss: 194.2499 - val_mae: 12.1953\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 170.6048 - mae: 12.2315 - val_loss: 180.5333 - val_mae: 12.6157\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 176.4674 - mae: 12.7130 - val_loss: 182.6526 - val_mae: 12.4376\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 173.3579 - mae: 12.4609 - val_loss: 190.6083 - val_mae: 12.2032\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 176.8059 - mae: 12.3373 - val_loss: 185.5657 - val_mae: 12.3032\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 172.8071 - mae: 12.1964 - val_loss: 200.6028 - val_mae: 12.1166\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 177.4419 - mae: 12.0694 - val_loss: 187.0646 - val_mae: 12.2301\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 178.9912 - mae: 12.7959 - val_loss: 184.5720 - val_mae: 12.9161\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 173.6610 - mae: 12.6125 - val_loss: 229.8958 - val_mae: 12.1644\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 185.8140 - mae: 11.9557 - val_loss: 220.8086 - val_mae: 12.1352\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 175.0917 - mae: 12.2005 - val_loss: 178.2728 - val_mae: 12.4723\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 173.4559 - mae: 12.5564 - val_loss: 185.4186 - val_mae: 12.1806\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 172.0094 - mae: 12.0822 - val_loss: 202.0582 - val_mae: 12.0587\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 178.1414 - mae: 12.5086 - val_loss: 177.8385 - val_mae: 12.4009\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 170.0106 - mae: 12.4190 - val_loss: 197.7555 - val_mae: 12.0257\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 186.3827 - mae: 12.1310 - val_loss: 198.7187 - val_mae: 12.0196\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 177.6135 - mae: 12.5651 - val_loss: 179.8604 - val_mae: 12.7582\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 178.3708 - mae: 12.5904 - val_loss: 207.8828 - val_mae: 12.0260\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 172.5477 - mae: 11.8985 - val_loss: 198.2751 - val_mae: 11.9882\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 168.2045 - mae: 11.9334 - val_loss: 176.9037 - val_mae: 12.2738\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 169.1226 - mae: 12.4042 - val_loss: 174.9080 - val_mae: 12.3715\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 168.3036 - mae: 12.2507 - val_loss: 194.7039 - val_mae: 11.9460\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 173.8390 - mae: 11.9631 - val_loss: 178.8678 - val_mae: 12.1271\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.3489 - mae: 12.2008 - val_loss: 174.0313 - val_mae: 12.3191\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 169.3732 - mae: 12.4613 - val_loss: 176.4502 - val_mae: 12.1594\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.7147 - mae: 11.8512 - val_loss: 220.5823 - val_mae: 11.9724\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 190.5334 - mae: 12.6737 - val_loss: 173.7976 - val_mae: 12.2219\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 165.2906 - mae: 11.9077 - val_loss: 218.1774 - val_mae: 11.9458\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 188.1682 - mae: 11.7464 - val_loss: 179.3272 - val_mae: 11.9851\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 177.9244 - mae: 12.8397 - val_loss: 173.0084 - val_mae: 12.4982\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 163.2081 - mae: 12.2020 - val_loss: 252.2213 - val_mae: 12.0759\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 185.2914 - mae: 11.5530 - val_loss: 174.2693 - val_mae: 12.0616\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 164.5068 - mae: 12.3216 - val_loss: 171.0381 - val_mae: 12.3993\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 176.6295 - mae: 12.3527 - val_loss: 182.3729 - val_mae: 11.8334\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 166.3105 - mae: 12.0535 - val_loss: 171.1426 - val_mae: 12.1103\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.2763 - mae: 12.3463 - val_loss: 178.8814 - val_mae: 11.8482\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 169.5690 - mae: 11.7872 - val_loss: 192.3802 - val_mae: 11.7785\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 165.3851 - mae: 11.7313 - val_loss: 172.7122 - val_mae: 11.9599\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.5684 - mae: 11.9047 - val_loss: 172.2102 - val_mae: 11.9517\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 160.5285 - mae: 11.9779 - val_loss: 168.0241 - val_mae: 12.1479\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 168.9258 - mae: 12.4548 - val_loss: 178.0706 - val_mae: 11.7567\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.1848 - mae: 11.4751 - val_loss: 218.8157 - val_mae: 11.7859\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.9446 - mae: 11.6753 - val_loss: 167.2962 - val_mae: 12.0603\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.7932 - mae: 12.3217 - val_loss: 169.6365 - val_mae: 11.9068\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 160.4237 - mae: 11.6547 - val_loss: 197.4283 - val_mae: 11.6990\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 160.8448 - mae: 11.6975 - val_loss: 166.5741 - val_mae: 11.9846\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 164.5160 - mae: 12.1874 - val_loss: 178.2603 - val_mae: 11.6554\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 158.7776 - mae: 11.6750 - val_loss: 176.1774 - val_mae: 11.6511\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 160.5621 - mae: 11.5714 - val_loss: 173.6385 - val_mae: 11.6708\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 164.8109 - mae: 12.0072 - val_loss: 164.0897 - val_mae: 12.0939\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 157.9275 - mae: 12.0238 - val_loss: 197.6802 - val_mae: 11.6262\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 166.9294 - mae: 11.3754 - val_loss: 176.7023 - val_mae: 11.5832\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 170.8251 - mae: 12.3672 - val_loss: 162.8431 - val_mae: 12.0315\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 155.1335 - mae: 11.9016 - val_loss: 216.5397 - val_mae: 11.6312\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 220.4865 - mae: 11.6232 - val_loss: 162.3183 - val_mae: 12.0534\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 193.7490 - mae: 13.4543 - val_loss: 166.4471 - val_mae: 12.2597\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 189.8974 - mae: 11.7720 - val_loss: 221.6590 - val_mae: 11.6250\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 219.4192 - mae: 13.8711 - val_loss: 272.1143 - val_mae: 16.0968\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 216.0445 - mae: 13.9115 - val_loss: 234.0439 - val_mae: 11.6859\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 164.4565 - mae: 11.2762 - val_loss: 166.6176 - val_mae: 12.2469\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 197.5102 - mae: 13.5913 - val_loss: 163.5862 - val_mae: 11.6210\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 183.3745 - mae: 11.5495 - val_loss: 242.7943 - val_mae: 11.7057\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 163.4604 - mae: 11.5836 - val_loss: 164.6029 - val_mae: 12.1789\n"
     ]
    }
   ],
   "source": [
    "#for v app\n",
    "# Using dense layer\n",
    "ANNmodel_v = Sequential()\n",
    "ANNmodel_v.add(Dense(128, input_dim=2, activation='relu'))\n",
    "#Output layer\n",
    "ANNmodel_v.add(Dense(1, activation='linear'))\n",
    "\n",
    "ANNmodel_v.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "ANNmodel_v.summary()\n",
    "\n",
    "history = ANNmodel_v.fit(features_train_v, y_train_list_v, validation_split=0.2, epochs =100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Prediction Based on ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the owner requirements:\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\n",
      "So here are the prediction of Engine Power and Vapp:\n",
      "Main Engine Power :  4343.9189453125 Kw\n",
      "Aux Engine Power :  466.1695861816406 Kw\n",
      "Vapp :  1.27 Knots\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('This is the owner requirements:')\n",
    "capacity_input = int(input('The capacity in DWT is:'))\n",
    "speed_input = float(input('The service speed in knots is:'))\n",
    "prediction_v = ANNmodel_v.predict([[capacity_input,speed_input]])\n",
    "prediction_v_score = float(prediction_v.item(0))\n",
    "prediction_m = ANNmodel_m.predict([[capacity_input,speed_input]])\n",
    "prediction_m_score = float(prediction_m.item(0))\n",
    "prediction_a = ANNmodel_a.predict([[capacity_input,speed_input]])\n",
    "prediction_a_score = float(prediction_a.item(0))\n",
    "print('')\n",
    "print('So here are the prediction of Engine Power and Vapp:')\n",
    "print('Main Engine Power : ',prediction_m_score,'Kw')\n",
    "print('Aux Engine Power : ',prediction_a_score,'Kw')\n",
    "print('Vapp : ',round(prediction_v_score,2),'Knots')\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
