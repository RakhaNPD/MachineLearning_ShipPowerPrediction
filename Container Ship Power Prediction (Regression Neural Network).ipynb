{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Nama Kapal</th>\n",
       "      <th>Flag State</th>\n",
       "      <th>Jenis Kapal</th>\n",
       "      <th>DWT</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>EHP ME</th>\n",
       "      <th>No ME</th>\n",
       "      <th>MCR ME</th>\n",
       "      <th>PME</th>\n",
       "      <th>...</th>\n",
       "      <th>Vapp</th>\n",
       "      <th>Vavg</th>\n",
       "      <th>Margin</th>\n",
       "      <th>Att. EEXI</th>\n",
       "      <th>Reference Line</th>\n",
       "      <th>Reduction Factor</th>\n",
       "      <th>Req. EEXI</th>\n",
       "      <th>Req - Att</th>\n",
       "      <th>Att/Req</th>\n",
       "      <th>Judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACACIA</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>Oil/Chemical Tanker</td>\n",
       "      <td>13565</td>\n",
       "      <td>13565</td>\n",
       "      <td>4900</td>\n",
       "      <td>1</td>\n",
       "      <td>4900</td>\n",
       "      <td>3675.00</td>\n",
       "      <td>...</td>\n",
       "      <td>13.144406</td>\n",
       "      <td>13.578394</td>\n",
       "      <td>0.678920</td>\n",
       "      <td>12.359438</td>\n",
       "      <td>11.730367</td>\n",
       "      <td>0.119563</td>\n",
       "      <td>10.327855</td>\n",
       "      <td>-2.031583</td>\n",
       "      <td>1.196709</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACADIA PARK</td>\n",
       "      <td>LIBERIA</td>\n",
       "      <td>Oil/Chemical Tanker</td>\n",
       "      <td>19996</td>\n",
       "      <td>19996</td>\n",
       "      <td>4900</td>\n",
       "      <td>1</td>\n",
       "      <td>4900</td>\n",
       "      <td>3675.00</td>\n",
       "      <td>...</td>\n",
       "      <td>12.486842</td>\n",
       "      <td>13.865005</td>\n",
       "      <td>0.693250</td>\n",
       "      <td>9.693722</td>\n",
       "      <td>9.706713</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>7.765856</td>\n",
       "      <td>-1.927866</td>\n",
       "      <td>1.248249</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ABRAO COCHIN</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>Container</td>\n",
       "      <td>23623</td>\n",
       "      <td>23623</td>\n",
       "      <td>15785</td>\n",
       "      <td>1</td>\n",
       "      <td>15785</td>\n",
       "      <td>11838.75</td>\n",
       "      <td>...</td>\n",
       "      <td>18.039638</td>\n",
       "      <td>13.989973</td>\n",
       "      <td>0.699499</td>\n",
       "      <td>18.152227</td>\n",
       "      <td>8.948391</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7.158713</td>\n",
       "      <td>-10.993514</td>\n",
       "      <td>2.535683</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACX CRYSTAL</td>\n",
       "      <td>PANAMA</td>\n",
       "      <td>Container</td>\n",
       "      <td>39565</td>\n",
       "      <td>39565</td>\n",
       "      <td>28880</td>\n",
       "      <td>1</td>\n",
       "      <td>28880</td>\n",
       "      <td>21660.00</td>\n",
       "      <td>...</td>\n",
       "      <td>20.608973</td>\n",
       "      <td>14.383796</td>\n",
       "      <td>0.719190</td>\n",
       "      <td>17.666437</td>\n",
       "      <td>6.957362</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.565890</td>\n",
       "      <td>-12.100548</td>\n",
       "      <td>3.174055</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>ACX DIAMOND</td>\n",
       "      <td>SINGAPORE</td>\n",
       "      <td>Container</td>\n",
       "      <td>39598</td>\n",
       "      <td>39598</td>\n",
       "      <td>28880</td>\n",
       "      <td>1</td>\n",
       "      <td>28880</td>\n",
       "      <td>21660.00</td>\n",
       "      <td>...</td>\n",
       "      <td>20.606700</td>\n",
       "      <td>14.384442</td>\n",
       "      <td>0.719222</td>\n",
       "      <td>17.653661</td>\n",
       "      <td>6.954532</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.563625</td>\n",
       "      <td>-12.090036</td>\n",
       "      <td>3.173050</td>\n",
       "      <td>Not Comply</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Status    Nama Kapal Flag State          Jenis Kapal    DWT  Capacity  \\\n",
       "0  ACTIVE        ACACIA     PANAMA  Oil/Chemical Tanker  13565     13565   \n",
       "1  ACTIVE   ACADIA PARK    LIBERIA  Oil/Chemical Tanker  19996     19996   \n",
       "2  ACTIVE  ABRAO COCHIN     PANAMA            Container  23623     23623   \n",
       "3  ACTIVE   ACX CRYSTAL     PANAMA            Container  39565     39565   \n",
       "4  ACTIVE   ACX DIAMOND  SINGAPORE            Container  39598     39598   \n",
       "\n",
       "   EHP ME  No ME  MCR ME       PME  ...       Vapp       Vavg    Margin  \\\n",
       "0    4900      1    4900   3675.00  ...  13.144406  13.578394  0.678920   \n",
       "1    4900      1    4900   3675.00  ...  12.486842  13.865005  0.693250   \n",
       "2   15785      1   15785  11838.75  ...  18.039638  13.989973  0.699499   \n",
       "3   28880      1   28880  21660.00  ...  20.608973  14.383796  0.719190   \n",
       "4   28880      1   28880  21660.00  ...  20.606700  14.384442  0.719222   \n",
       "\n",
       "   Att. EEXI  Reference Line  Reduction Factor  Req. EEXI  Req - Att  \\\n",
       "0  12.359438       11.730367          0.119563  10.327855  -2.031583   \n",
       "1   9.693722        9.706713          0.199950   7.765856  -1.927866   \n",
       "2  18.152227        8.948391          0.200000   7.158713 -10.993514   \n",
       "3  17.666437        6.957362          0.200000   5.565890 -12.100548   \n",
       "4  17.653661        6.954532          0.200000   5.563625 -12.090036   \n",
       "\n",
       "    Att/Req    Judgment  \n",
       "0  1.196709  Not Comply  \n",
       "1  1.248249  Not Comply  \n",
       "2  2.535683  Not Comply  \n",
       "3  3.174055  Not Comply  \n",
       "4  3.173050  Not Comply  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdata = pd.read_csv(\"Excel Perhitungan EEXI_Container Ship Trial Data.csv\")\n",
    "inputdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features :\n",
      " [[1.35650e+04 1.00000e+01]\n",
      " [1.99960e+04 5.30000e+00]\n",
      " [2.36230e+04 2.30000e+01]\n",
      " [3.95650e+04 2.53000e+01]\n",
      " [3.95980e+04 2.48000e+01]\n",
      " [3.95800e+04 2.57000e+01]\n",
      " [6.69400e+04 2.88000e+01]\n",
      " [5.13140e+04 2.64000e+01]\n",
      " [1.77320e+04 2.00000e+01]\n",
      " [2.19720e+04 2.15000e+01]\n",
      " [9.07990e+04 2.90000e+01]\n",
      " [1.17930e+04 1.74000e+01]\n",
      " [9.06470e+04 2.91000e+01]\n",
      " [7.24470e+04 2.88000e+01]\n",
      " [7.24470e+04 2.89000e+01]\n",
      " [7.29120e+04 2.89000e+01]\n",
      " [7.29120e+04 2.86000e+01]\n",
      " [6.69400e+04 2.88000e+01]\n",
      " [6.15300e+03 9.90000e+00]\n",
      " [8.11710e+04 2.71000e+01]\n",
      " [3.33040e+04 2.45000e+01]\n",
      " [8.11710e+04 2.73000e+01]\n",
      " [2.47270e+04 1.65000e+01]\n",
      " [2.92660e+04 2.27000e+01]\n",
      " [5.13140e+04 2.65000e+01]\n",
      " [5.24520e+04 2.62000e+01]\n",
      " [2.47090e+04 1.88000e+01]\n",
      " [5.21180e+04 2.74000e+01]\n",
      " [5.21180e+04 2.83000e+01]\n",
      " [5.21180e+04 2.72000e+01]\n",
      " [2.19730e+04 2.17000e+01]\n",
      " [1.12020e+04 1.28000e+01]\n",
      " [1.20830e+04 1.31000e+01]\n",
      " [1.11700e+04 1.44000e+01]\n",
      " [7.20200e+03 1.54000e+01]\n",
      " [5.15330e+04 1.98000e+01]\n",
      " [7.29820e+04 2.91000e+01]\n",
      " [1.11700e+04 1.25000e+01]\n",
      " [1.19680e+04 1.62000e+01]\n",
      " [6.83200e+03 1.14000e+01]\n",
      " [3.92660e+04 1.78000e+01]\n",
      " [1.19830e+04 1.48000e+01]\n",
      " [7.28070e+04 2.88000e+01]\n",
      " [7.29680e+04 2.81000e+01]\n",
      " [6.23100e+03 1.35000e+01]\n",
      " [2.47690e+04 1.80000e+01]\n",
      " [3.29710e+04 1.68000e+01]\n",
      " [6.69400e+04 2.87000e+01]\n",
      " [2.84510e+04 1.32000e+01]\n",
      " [1.78520e+04 2.08000e+01]\n",
      " [3.57290e+04 1.65000e+01]\n",
      " [5.20550e+04 2.73000e+01]\n",
      " [1.16560e+04 1.35000e+01]\n",
      " [2.14420e+04 2.17000e+01]\n",
      " [3.34430e+04 2.37000e+01]\n",
      " [2.14230e+04 2.14000e+01]\n",
      " [8.05470e+04 2.28000e+01]\n",
      " [1.31095e+05 1.75000e+01]\n",
      " [1.31095e+05 1.65000e+01]\n",
      " [1.31095e+05 1.79000e+01]\n",
      " [1.31095e+05 1.72000e+01]\n",
      " [8.05510e+04 2.26000e+01]\n",
      " [2.47730e+04 1.81000e+01]\n",
      " [1.16851e+05 2.25000e+01]\n",
      " [1.18170e+04 1.38000e+01]\n",
      " [3.88970e+04 1.91000e+01]\n",
      " [3.88830e+04 1.51000e+01]\n",
      " [2.20810e+04 1.45000e+01]\n",
      " [3.89040e+04 1.99000e+01]\n",
      " [2.20810e+04 1.52000e+01]\n",
      " [2.20810e+04 1.63000e+01]\n",
      " [3.89210e+04 1.64000e+01]\n",
      " [3.89380e+04 1.73000e+01]\n",
      " [2.20810e+04 1.55000e+01]\n",
      " [1.36840e+04 1.81000e+01]\n",
      " [1.37010e+04 1.66000e+01]\n",
      " [1.37070e+04 1.76000e+01]\n",
      " [1.36840e+04 1.69000e+01]\n",
      " [1.36730e+04 1.99000e+01]\n",
      " [1.36860e+04 1.39000e+01]\n",
      " [1.37090e+04 1.08000e+01]\n",
      " [1.36870e+04 1.82000e+01]\n",
      " [1.37060e+04 1.74000e+01]\n",
      " [3.71150e+04 2.42000e+01]\n",
      " [2.19350e+04 2.15000e+01]\n",
      " [1.16851e+05 1.76000e+01]\n",
      " [1.31770e+04 1.52000e+01]\n",
      " [1.56390e+04 1.96000e+01]\n",
      " [1.72240e+04 2.02000e+01]\n",
      " [3.93570e+04 1.76000e+01]\n",
      " [6.59500e+04 2.76000e+01]\n",
      " [7.44530e+04 2.88000e+01]\n",
      " [8.85300e+03 1.70000e+01]\n",
      " [6.83200e+03 1.65000e+01]\n",
      " [6.27300e+03 1.48000e+01]\n",
      " [2.49240e+04 2.19000e+01]\n",
      " [1.77040e+04 1.99000e+01]\n",
      " [2.49730e+04 2.23000e+01]\n",
      " [1.77050e+04 2.03000e+01]\n",
      " [2.33730e+04 1.74000e+01]\n",
      " [7.13100e+03 1.48000e+01]\n",
      " [2.58640e+04 1.80000e+01]\n",
      " [1.18170e+04 1.56000e+01]\n",
      " [1.17860e+04 2.00000e+01]\n",
      " [2.41960e+05 1.89000e+01]\n",
      " [2.41960e+05 2.17000e+01]\n",
      " [2.41960e+05 1.69000e+01]\n",
      " [2.41960e+05 1.60000e+01]\n",
      " [3.73010e+04 1.74000e+01]\n",
      " [3.73010e+04 2.09000e+01]\n",
      " [3.73010e+04 1.98000e+01]\n",
      " [3.73010e+04 1.61000e+01]\n",
      " [3.69720e+04 2.04000e+01]\n",
      " [3.69720e+04 1.63000e+01]\n",
      " [3.69720e+04 2.03000e+01]\n",
      " [3.69720e+04 2.02000e+01]\n",
      " [3.73010e+04 1.93000e+01]\n",
      " [3.69720e+04 1.95000e+01]\n",
      " [2.22170e+04 1.62000e+01]\n",
      " [2.23830e+04 1.75000e+01]\n",
      " [2.21490e+04 1.20000e+01]\n",
      " [2.23980e+04 1.68000e+01]\n",
      " [2.20530e+04 1.90000e+01]\n",
      " [2.23900e+04 1.69000e+01]\n",
      " [2.23820e+04 1.32000e+01]\n",
      " [2.20500e+04 1.91000e+01]\n",
      " [2.24050e+04 1.75000e+01]\n",
      " [2.23830e+04 1.57000e+01]\n",
      " [2.23890e+04 1.56000e+01]\n",
      " [2.23770e+04 1.64000e+01]\n",
      " [2.23770e+04 1.84000e+01]\n",
      " [2.23770e+04 1.80000e+01]\n",
      " [2.24000e+04 1.69000e+01]\n",
      " [2.21490e+04 1.70000e+01]\n",
      " [2.22170e+04 1.97000e+01]\n",
      " [2.22170e+04 1.80000e+01]\n",
      " [2.21490e+04 1.59000e+01]\n",
      " [2.21490e+04 1.31000e+01]\n",
      " [2.23630e+04 1.95000e+01]\n",
      " [1.30573e+05 2.20000e+01]\n",
      " [1.30573e+05 2.35000e+01]\n",
      " [1.30573e+05 2.31000e+01]\n",
      " [1.30573e+05 1.59000e+01]\n",
      " [1.30573e+05 2.37000e+01]\n",
      " [1.30573e+05 1.74000e+01]\n",
      " [1.98886e+05 1.35000e+01]\n",
      " [1.98937e+05 1.77000e+01]\n",
      " [1.98886e+05 1.21000e+01]\n",
      " [1.98937e+05 1.60000e+01]\n",
      " [1.03668e+05 2.64000e+01]\n",
      " [1.03518e+05 2.60000e+01]\n",
      " [1.03723e+05 2.59000e+01]\n",
      " [1.03668e+05 2.61000e+01]\n",
      " [1.03891e+05 2.65000e+01]\n",
      " [1.03668e+05 2.58000e+01]\n",
      " [1.03891e+05 2.70000e+01]\n",
      " [1.03891e+05 2.71000e+01]\n",
      " [1.56238e+05 2.19000e+01]\n",
      " [1.56174e+05 2.24000e+01]\n",
      " [3.29630e+04 1.55000e+01]\n",
      " [3.35410e+04 1.88000e+01]\n",
      " [3.35410e+04 1.81000e+01]\n",
      " [3.28310e+04 1.85000e+01]\n",
      " [3.35010e+04 1.74000e+01]\n",
      " [3.35770e+04 1.82000e+01]\n",
      " [3.28120e+04 2.09000e+01]\n",
      " [3.29260e+04 1.67000e+01]\n",
      " [3.35410e+04 1.89000e+01]\n",
      " [3.35440e+04 1.83000e+01]\n",
      " [3.34960e+04 1.69000e+01]\n",
      " [1.93090e+04 2.15000e+01]\n",
      " [1.93090e+04 2.14000e+01]\n",
      " [1.93110e+04 2.12000e+01]\n",
      " [7.82680e+04 1.72000e+01]\n",
      " [7.82680e+04 1.82000e+01]\n",
      " [1.52344e+05 1.81000e+01]\n",
      " [6.32160e+04 1.79000e+01]\n",
      " [6.32160e+04 1.85000e+01]\n",
      " [6.32160e+04 1.74000e+01]\n",
      " [6.32160e+04 2.02000e+01]\n",
      " [6.32160e+04 2.04000e+01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_capacity_array = np.array(inputdata['Capacity'])\n",
    "feature_speed_array = np.array(inputdata['Vs'])\n",
    "\n",
    "features = np.stack((feature_capacity_array,feature_speed_array),axis=1)\n",
    "print('features :\\n',features)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target PME :\n",
      " [ 3675.    3675.   11838.75 21660.   21660.   21660.   42900.   27120.\n",
      "  7220.25 11865.   48075.    5175.   48075.   47190.   47190.   47190.\n",
      " 47190.   42900.    1631.25 46147.5  16301.25 46012.5   7785.   13547.25\n",
      " 27120.   25290.    7785.   30510.   30510.   30510.   11865.    6300.\n",
      "  7200.    6300.    3975.   27120.   47190.    6300.    7200.    1575.\n",
      " 18952.5   7200.   47190.   47190.    3150.    7785.   18952.5  42900.\n",
      "  9000.    9480.   10650.   30510.    4440.   11865.   16293.75 11865.\n",
      " 20295.   30502.5  30502.5  30502.5  30502.5  20295.    7785.   31110.\n",
      "  5175.   12690.   12690.   10050.   12690.   10050.   10050.   12690.\n",
      " 12690.   10050.    5212.5   5212.5   5212.5   5212.5   5212.5   5212.5\n",
      "  5212.5   5212.5   5212.5  16301.25 11865.   31110.    3330.    6420.75\n",
      "  7220.25 18952.5  38542.5  45292.5   3353.25  2514.75  2520.   10314.75\n",
      "  7220.25 10314.75  7220.25 12495.    3975.   12495.    5475.    5475.\n",
      " 43950.   43950.   43950.   43950.   18195.   18195.   18195.   18195.\n",
      " 18195.   18195.   18195.   18195.   18195.   18195.   10050.    8790.\n",
      "  8970.    8970.    8925.    8970.    8970.    8925.    8970.    8790.\n",
      "  8790.    8790.    8790.    8790.    8790.    8970.   10050.   10050.\n",
      "  8970.    8970.    8790.   30810.   30810.   30810.   30810.   30810.\n",
      " 30810.   44475.   44475.   44475.   44475.   42052.5  42052.5  42052.5\n",
      " 42052.5  42052.5  42052.5  42052.5  42052.5  36375.   36375.   11250.\n",
      " 10612.5  10612.5  11250.   10612.5  10612.5  11250.   11250.   10612.5\n",
      " 10612.5  10612.5   9360.    9360.    9360.   41175.   41175.   39937.5\n",
      " 36450.   36450.   36450.   36450.   36450.  ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_pme = np.array(inputdata['PME'])\n",
    "print('target PME :\\n',target_pme)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target PAE :\n",
      " [  43.8903   367.5     1091.9375  2374.5     2374.5     2374.5\n",
      " 3967.5     2784.      1083.0375  1639.875   4355.625    776.25\n",
      " 4355.625   4289.25    4289.25    4289.25    4289.25    3967.5\n",
      "  244.6875  4211.0625  1972.59375 4200.9375  1167.75    1766.04375\n",
      " 2784.      1764.5     1167.75    3038.25    3038.25    3038.25\n",
      " 1639.875    945.      1080.       945.       596.25    2784.\n",
      " 4289.25     945.      1080.       236.25    2171.4375  1080.\n",
      " 4289.25    4289.25     472.5     1167.75    2171.4375  3967.5\n",
      " 1350.      1422.      1548.75    3038.25     666.      1639.875\n",
      " 1972.03125 1639.875   2272.125   3037.6875  3037.6875  3037.6875\n",
      " 3037.6875  2272.125   1167.75    3083.25     776.25    1701.75\n",
      " 1701.75    1503.75    1701.75    1503.75    1503.75    1701.75\n",
      " 1701.75    1503.75     781.875    781.875    781.875    781.875\n",
      "  781.875    781.875    781.875    781.875    781.875   1972.59375\n",
      " 1639.875   3083.25     499.5      963.1125  1083.0375  2171.4375\n",
      " 3640.6875  4146.9375   502.9875   377.2125   378.      1523.60625\n",
      " 1083.0375  1523.60625 1083.0375  1687.125    596.25    1687.125\n",
      "  821.25     821.25    4046.25    4046.25    4046.25    4046.25\n",
      " 2114.625   2114.625   2114.625   2114.625   2114.625   2114.625\n",
      " 2114.625   2114.625   2114.625   2114.625   1503.75    1318.5\n",
      " 1345.5     1345.5     1338.75    1345.5     1345.5     1338.75\n",
      " 1345.5     1318.5     1318.5     1318.5     1318.5     1318.5\n",
      " 1318.5     1345.5     1503.75    1503.75    1345.5     1345.5\n",
      " 1318.5     3060.75    3060.75    3060.75    3060.75    3060.75\n",
      " 3060.75    4085.625   4085.625   4085.625   4085.625   3903.9375\n",
      " 3903.9375  3903.9375  3903.9375  3903.9375  3903.9375  3903.9375\n",
      " 3903.9375  3478.125   3478.125   1593.75    1545.9375  1545.9375\n",
      " 1593.75    1545.9375  1545.9375  1593.75    1593.75    1545.9375\n",
      " 1545.9375  1545.9375  1404.      1404.      1404.      3838.125\n",
      " 3838.125   3745.3125  3483.75    3483.75    3483.75    3483.75\n",
      " 3483.75   ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_pae = np.array(inputdata['PAE'])\n",
    "print('target PAE :\\n',target_pae)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target Vapp :\n",
      " [13.14440569 12.48684212 18.03963759 20.60897256 20.60670023 20.60793942\n",
      " 24.14266635 21.46174552 15.88983737 18.2267802  24.08587329 15.00833684\n",
      " 24.091211   24.66276708 24.66276708 24.6419069  24.6419069  24.14266635\n",
      " 11.13192003 24.11442826 19.17808527 24.09089049 15.59272086 18.34168758\n",
      " 21.46174552 20.90699046 15.59422269 22.27526614 22.27526614 22.27526614\n",
      " 18.22667049 16.13475679 16.70108084 16.14086252 14.67124809 21.44966061\n",
      " 24.6387797  16.14086252 16.72221742 10.8511952  19.73155955 16.71944747\n",
      " 24.64660407 24.63940487 13.83916433 15.58922141 20.1928514  24.14266635\n",
      " 16.06418331 17.38404161 16.48710542 22.27882976 14.28327982 18.285736\n",
      " 19.16458395 18.28788006 18.35694994 19.71539596 19.71539596 19.71539596\n",
      " 19.71539596 18.35682938 15.58888848 20.1496298  15.0043019  17.28364924\n",
      " 17.28447215 17.23422758 17.28323791 17.23422758 17.23422758 17.28223931\n",
      " 17.28124121 17.23422758 14.75147716 14.74905511 14.74820108 14.75147716\n",
      " 14.75304619 14.75119204 14.74791649 14.75104949 14.74834339 18.90523862\n",
      " 18.23084345 20.1496298  12.7684089  15.53620927 15.95104064 19.72551959\n",
      " 23.34184869 24.23995291 13.48926204 12.68281514 12.83573271 17.10800898\n",
      " 15.89315881 17.10356561 15.89304008 18.39285086 14.6904845  18.1481449\n",
      " 15.28881118 15.29412359 20.53425203 20.53425203 20.53425203 20.53425203\n",
      " 19.59771406 19.59771406 19.59771406 19.59771406 19.62069011 19.62069011\n",
      " 19.62069011 19.62069011 19.59771406 19.62069011 17.22023753 16.45201786\n",
      " 16.58659869 16.56209287 16.56833089 16.5628754  16.56365824 16.568629\n",
      " 16.56140841 16.45201786 16.45143468 16.45260122 16.45260122 16.45260122\n",
      " 16.45036597 16.58659869 17.22023753 17.22023753 16.58659869 16.58659869\n",
      " 16.45396308 19.79186675 19.79186675 19.79186675 19.79186675 19.79186675\n",
      " 19.79186675 21.15719857 21.15648115 21.15719857 21.15648115 22.6346298\n",
      " 22.63896485 22.63304207 22.6346298  22.62819815 22.6346298  22.62819815\n",
      " 22.62819815 20.4275679  20.42867485 16.9709802  16.60594308 16.60594308\n",
      " 16.97998881 16.60856403 16.60358725 16.98128888 16.97350121 16.60594308\n",
      " 16.60574665 16.60889189 17.13169055 17.13169055 17.13145588 23.32727297\n",
      " 23.32727297 21.14425208 23.04020687 23.04020687 23.04020687 23.04020687\n",
      " 23.04020687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_vapp = np.array(inputdata['Vapp'])\n",
    "print('target Vapp :\\n',target_vapp)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Train and Test Data using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For main engine\n",
    "features_train_m,features_test_m,y_train_list_m,y_test_list_m = train_test_split(features,target_pme,test_size=0.25,random_state=20)\n",
    "\n",
    "\n",
    "# For aux engine\n",
    "features_train_a,features_test_a,y_train_list_a,y_test_list_a = train_test_split(features,target_pae,test_size=0.25,random_state=20)\n",
    "\n",
    "# For Vapp\n",
    "features_train_v,features_test_v,y_train_list_v,y_test_list_v = train_test_split(features,target_vapp,test_size=0.25,random_state=20)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train_m)\n",
    "\n",
    "features_train_m_scaled = scaler.transform(features_train_m)\n",
    "features_test_m_scaled = scaler.transform(features_test_m)\n",
    "features_train_a_scaled = scaler.transform(features_train_a)\n",
    "features_test_a_scaled = scaler.transform(features_test_a)\n",
    "features_train_v_scaled = scaler.transform(features_train_v)\n",
    "features_test_v_scaled = scaler.transform(features_test_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Artificial Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 128)               384       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513 (2.00 KB)\n",
      "Trainable params: 513 (2.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 48ms/step - loss: 805301696.0000 - mae: 22492.4629 - val_loss: 975775040.0000 - val_mae: 26193.2773\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 676450432.0000 - mae: 20615.5898 - val_loss: 795534912.0000 - val_mae: 23695.8418\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 557842240.0000 - mae: 18746.1699 - val_loss: 641011968.0000 - val_mae: 21259.7461\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 451640544.0000 - mae: 16881.1719 - val_loss: 514326592.0000 - val_mae: 18957.9453\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 366084960.0000 - mae: 15135.5352 - val_loss: 410873792.0000 - val_mae: 16759.9160\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 298292576.0000 - mae: 13565.8955 - val_loss: 328275904.0000 - val_mae: 14666.9678\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 236082320.0000 - mae: 11906.6250 - val_loss: 265482672.0000 - val_mae: 12723.8105\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 194312048.0000 - mae: 10565.2207 - val_loss: 218811248.0000 - val_mae: 11062.0225\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 158709264.0000 - mae: 9255.2256 - val_loss: 187265232.0000 - val_mae: 9907.9131\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 133435704.0000 - mae: 8146.6167 - val_loss: 166837488.0000 - val_mae: 9155.6318\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 115254272.0000 - mae: 7540.1587 - val_loss: 154783248.0000 - val_mae: 8971.8770\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 103649240.0000 - mae: 7150.1396 - val_loss: 149053856.0000 - val_mae: 8878.1455\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 97647176.0000 - mae: 6942.3325 - val_loss: 147879888.0000 - val_mae: 8795.7793\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91880768.0000 - mae: 6726.2568 - val_loss: 149521008.0000 - val_mae: 8730.3193\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90806080.0000 - mae: 6597.5811 - val_loss: 152663520.0000 - val_mae: 8683.2432\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90305864.0000 - mae: 6466.4116 - val_loss: 155774352.0000 - val_mae: 8690.5547\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89951672.0000 - mae: 6388.3394 - val_loss: 158161440.0000 - val_mae: 8703.9180\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90393752.0000 - mae: 6361.8193 - val_loss: 160147952.0000 - val_mae: 8717.5166\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91276288.0000 - mae: 6348.0957 - val_loss: 161179456.0000 - val_mae: 8724.1172\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90985632.0000 - mae: 6340.5718 - val_loss: 159895888.0000 - val_mae: 8715.7422\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90778936.0000 - mae: 6359.7563 - val_loss: 158769440.0000 - val_mae: 8708.0137\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90498320.0000 - mae: 6365.0376 - val_loss: 158152320.0000 - val_mae: 8703.5859\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90407728.0000 - mae: 6375.5513 - val_loss: 157360064.0000 - val_mae: 8697.7217\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90258840.0000 - mae: 6385.2603 - val_loss: 156325968.0000 - val_mae: 8691.1426\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90142432.0000 - mae: 6402.1040 - val_loss: 155431904.0000 - val_mae: 8689.1924\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90143072.0000 - mae: 6423.8818 - val_loss: 154548512.0000 - val_mae: 8687.1543\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90181680.0000 - mae: 6443.7227 - val_loss: 153999936.0000 - val_mae: 8685.7949\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90138112.0000 - mae: 6451.0972 - val_loss: 153900656.0000 - val_mae: 8685.4805\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90229560.0000 - mae: 6457.5659 - val_loss: 153644176.0000 - val_mae: 8684.7803\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90127616.0000 - mae: 6458.1885 - val_loss: 153876720.0000 - val_mae: 8685.2676\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90119064.0000 - mae: 6450.4834 - val_loss: 154313328.0000 - val_mae: 8686.2266\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90225448.0000 - mae: 6436.7324 - val_loss: 155081136.0000 - val_mae: 8687.8867\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 90455000.0000 - mae: 6421.5137 - val_loss: 155984704.0000 - val_mae: 8689.7363\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90533528.0000 - mae: 6405.2285 - val_loss: 156878224.0000 - val_mae: 8693.4072\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90293504.0000 - mae: 6387.4219 - val_loss: 157246880.0000 - val_mae: 8696.1396\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90340296.0000 - mae: 6381.3394 - val_loss: 157741200.0000 - val_mae: 8699.7422\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 90411120.0000 - mae: 6375.3818 - val_loss: 157834160.0000 - val_mae: 8700.3545\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90430856.0000 - mae: 6384.8115 - val_loss: 156859872.0000 - val_mae: 8693.0068\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 90350320.0000 - mae: 6395.0098 - val_loss: 156143056.0000 - val_mae: 8689.5537\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 90310696.0000 - mae: 6401.9458 - val_loss: 156090496.0000 - val_mae: 8689.3594\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 90226736.0000 - mae: 6410.3135 - val_loss: 155116208.0000 - val_mae: 8687.1963\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 90140920.0000 - mae: 6422.0601 - val_loss: 154916624.0000 - val_mae: 8686.6621\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90220848.0000 - mae: 6436.9058 - val_loss: 153841440.0000 - val_mae: 8684.0820\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90109496.0000 - mae: 6456.0312 - val_loss: 153361376.0000 - val_mae: 8682.8115\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90130752.0000 - mae: 6465.6064 - val_loss: 153096928.0000 - val_mae: 8682.0449\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90159880.0000 - mae: 6470.4409 - val_loss: 153045120.0000 - val_mae: 8681.8164\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90222120.0000 - mae: 6466.0620 - val_loss: 153460880.0000 - val_mae: 8682.7773\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 90342816.0000 - mae: 6473.0825 - val_loss: 152866416.0000 - val_mae: 8681.1582\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 90212664.0000 - mae: 6479.4502 - val_loss: 152774384.0000 - val_mae: 8680.8174\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 90169256.0000 - mae: 6475.6714 - val_loss: 153027744.0000 - val_mae: 8681.3818\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90123168.0000 - mae: 6464.0293 - val_loss: 153597072.0000 - val_mae: 8682.7178\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90258648.0000 - mae: 6451.4375 - val_loss: 154045136.0000 - val_mae: 8683.6982\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90314456.0000 - mae: 6456.8853 - val_loss: 153716656.0000 - val_mae: 8682.8047\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90049040.0000 - mae: 6447.3706 - val_loss: 154489456.0000 - val_mae: 8684.5186\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 90072624.0000 - mae: 6432.6133 - val_loss: 154792224.0000 - val_mae: 8685.0996\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 90221344.0000 - mae: 6424.0449 - val_loss: 155545824.0000 - val_mae: 8686.6396\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90207704.0000 - mae: 6416.1177 - val_loss: 155535808.0000 - val_mae: 8686.5166\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90160112.0000 - mae: 6414.6597 - val_loss: 155446816.0000 - val_mae: 8686.2246\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90094808.0000 - mae: 6411.0024 - val_loss: 155946432.0000 - val_mae: 8687.1748\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90179744.0000 - mae: 6396.1880 - val_loss: 156615344.0000 - val_mae: 8689.4648\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90245960.0000 - mae: 6391.0459 - val_loss: 156502320.0000 - val_mae: 8688.5078\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90288880.0000 - mae: 6397.8438 - val_loss: 156296880.0000 - val_mae: 8687.5820\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90247936.0000 - mae: 6401.2583 - val_loss: 155871424.0000 - val_mae: 8686.6064\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90220680.0000 - mae: 6408.7661 - val_loss: 155730912.0000 - val_mae: 8686.2100\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 90179128.0000 - mae: 6403.2070 - val_loss: 156236064.0000 - val_mae: 8687.1484\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90209136.0000 - mae: 6397.1504 - val_loss: 156268560.0000 - val_mae: 8687.1113\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90266728.0000 - mae: 6396.1182 - val_loss: 156051360.0000 - val_mae: 8686.5635\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90150584.0000 - mae: 6400.6997 - val_loss: 155617392.0000 - val_mae: 8685.5547\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90123056.0000 - mae: 6409.7534 - val_loss: 155422288.0000 - val_mae: 8685.0381\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90194264.0000 - mae: 6416.5308 - val_loss: 155342736.0000 - val_mae: 8684.7637\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90140568.0000 - mae: 6405.4854 - val_loss: 156326432.0000 - val_mae: 8686.7041\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90182896.0000 - mae: 6388.4038 - val_loss: 156918144.0000 - val_mae: 8690.8271\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90341232.0000 - mae: 6382.9663 - val_loss: 156911344.0000 - val_mae: 8690.6914\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90421600.0000 - mae: 6393.8682 - val_loss: 155942576.0000 - val_mae: 8685.5869\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90151776.0000 - mae: 6399.7939 - val_loss: 155677632.0000 - val_mae: 8684.9238\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90097088.0000 - mae: 6407.9238 - val_loss: 155052032.0000 - val_mae: 8683.4697\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90040184.0000 - mae: 6417.9854 - val_loss: 154313248.0000 - val_mae: 8681.7002\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89947720.0000 - mae: 6437.7856 - val_loss: 152985472.0000 - val_mae: 8678.3584\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90162128.0000 - mae: 6483.3887 - val_loss: 151566320.0000 - val_mae: 8680.8086\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 90267936.0000 - mae: 6514.7363 - val_loss: 151061936.0000 - val_mae: 8688.6836\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90443488.0000 - mae: 6540.0728 - val_loss: 150472816.0000 - val_mae: 8698.7002\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90549128.0000 - mae: 6556.6440 - val_loss: 150455472.0000 - val_mae: 8698.8037\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90683424.0000 - mae: 6553.3252 - val_loss: 151129936.0000 - val_mae: 8686.9268\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90355472.0000 - mae: 6528.6367 - val_loss: 151052720.0000 - val_mae: 8687.9756\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90311744.0000 - mae: 6523.3013 - val_loss: 151649856.0000 - val_mae: 8678.6953\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 90099040.0000 - mae: 6497.9585 - val_loss: 152363104.0000 - val_mae: 8675.7812\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89984872.0000 - mae: 6470.5605 - val_loss: 153635088.0000 - val_mae: 8678.9326\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89848792.0000 - mae: 6429.0479 - val_loss: 155375776.0000 - val_mae: 8682.7695\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89983832.0000 - mae: 6405.5229 - val_loss: 156389472.0000 - val_mae: 8685.2480\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90071040.0000 - mae: 6384.3081 - val_loss: 157342736.0000 - val_mae: 8692.4023\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90292296.0000 - mae: 6376.2720 - val_loss: 157427184.0000 - val_mae: 8692.9355\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 90289216.0000 - mae: 6371.0664 - val_loss: 157647440.0000 - val_mae: 8694.4600\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90557240.0000 - mae: 6368.3179 - val_loss: 157951120.0000 - val_mae: 8696.5693\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90194280.0000 - mae: 6379.7490 - val_loss: 155408528.0000 - val_mae: 8682.1445\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 89765488.0000 - mae: 6407.3677 - val_loss: 153581392.0000 - val_mae: 8677.8838\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 91386128.0000 - mae: 6494.5850 - val_loss: 150955696.0000 - val_mae: 8687.0205\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90292256.0000 - mae: 6532.8315 - val_loss: 150601744.0000 - val_mae: 8692.8740\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 90418016.0000 - mae: 6544.5122 - val_loss: 150466656.0000 - val_mae: 8695.0537\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90505328.0000 - mae: 6554.2617 - val_loss: 150449024.0000 - val_mae: 8695.1416\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 90411960.0000 - mae: 6540.7617 - val_loss: 151526384.0000 - val_mae: 8677.0918\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#for main engine\n",
    "# Using dense layer\n",
    "ANNmodel_m = Sequential()\n",
    "ANNmodel_m.add(Dense(128, input_dim=2, activation='relu'))\n",
    "#Output layer\n",
    "ANNmodel_m.add(Dense(1, activation='linear'))\n",
    "\n",
    "ANNmodel_m.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "ANNmodel_m.summary()\n",
    "\n",
    "history = ANNmodel_m.fit(features_train_m, y_train_list_m, validation_split=0.2, epochs =100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 128)               384       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513 (2.00 KB)\n",
      "Trainable params: 513 (2.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 51ms/step - loss: 31983556.0000 - mae: 3377.3640 - val_loss: 14772857.0000 - val_mae: 2134.0049\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 3204932.5000 - mae: 879.5258 - val_loss: 1533868.1250 - val_mae: 1055.4263\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2458877.0000 - mae: 1315.9877 - val_loss: 7227546.0000 - val_mae: 2419.0439\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 6094492.5000 - mae: 2131.2751 - val_loss: 7166785.0000 - val_mae: 2409.4512\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 4637327.0000 - mae: 1864.5265 - val_loss: 2518501.7500 - val_mae: 1379.5946\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1642682.1250 - mae: 1075.7053 - val_loss: 1773504.2500 - val_mae: 1000.5341\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1167581.8750 - mae: 772.2610 - val_loss: 3402664.0000 - val_mae: 1044.0491\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1899073.5000 - mae: 763.8541 - val_loss: 3618282.2500 - val_mae: 1052.3954\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1755747.0000 - mae: 765.9274 - val_loss: 2276124.2500 - val_mae: 1003.2679\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1158756.5000 - mae: 802.2331 - val_loss: 1526251.2500 - val_mae: 1018.7507\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1043529.0625 - mae: 884.1086 - val_loss: 1524710.2500 - val_mae: 1051.8907\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1195974.3750 - mae: 940.5403 - val_loss: 1513001.6250 - val_mae: 1046.7474\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1119352.8750 - mae: 909.9843 - val_loss: 1517881.6250 - val_mae: 1019.7321\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1033102.0625 - mae: 854.7214 - val_loss: 1706044.6250 - val_mae: 1000.6359\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1061722.2500 - mae: 822.9762 - val_loss: 1721428.6250 - val_mae: 1000.1720\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1048834.3750 - mae: 829.6486 - val_loss: 1619465.3750 - val_mae: 1006.7792\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1028533.3125 - mae: 847.7881 - val_loss: 1524095.1250 - val_mae: 1017.6227\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1078933.7500 - mae: 893.5476 - val_loss: 1495244.5000 - val_mae: 1025.4623\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1060607.1250 - mae: 886.4413 - val_loss: 1545174.8750 - val_mae: 1013.9429\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1040994.5000 - mae: 849.8398 - val_loss: 1743620.6250 - val_mae: 999.0262\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1086502.6250 - mae: 811.9996 - val_loss: 1766306.5000 - val_mae: 998.5245\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1099123.8750 - mae: 843.3450 - val_loss: 1551215.7500 - val_mae: 1012.4490\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1033734.0000 - mae: 860.7457 - val_loss: 1541882.6250 - val_mae: 1013.4252\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1039555.3125 - mae: 862.9603 - val_loss: 1575874.5000 - val_mae: 1009.1581\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1071210.5000 - mae: 851.3687 - val_loss: 1679936.8750 - val_mae: 1000.1687\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1049496.0000 - mae: 825.5919 - val_loss: 1707143.7500 - val_mae: 998.5470\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1057509.6250 - mae: 821.2016 - val_loss: 1669485.6250 - val_mae: 1000.4694\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1041810.5000 - mae: 830.3550 - val_loss: 1603925.1250 - val_mae: 1005.5024\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1044687.6875 - mae: 858.0904 - val_loss: 1523593.3750 - val_mae: 1014.5845\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1046300.5000 - mae: 872.7553 - val_loss: 1550664.8750 - val_mae: 1010.5280\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1090743.7500 - mae: 845.0704 - val_loss: 1706663.7500 - val_mae: 997.6241\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1051161.7500 - mae: 834.7358 - val_loss: 1548323.3750 - val_mae: 1010.2781\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1032438.6875 - mae: 859.2367 - val_loss: 1541755.0000 - val_mae: 1010.8381\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1058029.8750 - mae: 856.0284 - val_loss: 1549146.6250 - val_mae: 1009.6317\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1027973.9375 - mae: 869.4927 - val_loss: 1486517.6250 - val_mae: 1022.7515\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1080556.0000 - mae: 895.6856 - val_loss: 1507177.8750 - val_mae: 1015.4327\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1024413.3125 - mae: 853.2762 - val_loss: 1720114.6250 - val_mae: 996.1876\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1073283.8750 - mae: 835.5383 - val_loss: 1671630.8750 - val_mae: 997.5989\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1038786.3125 - mae: 830.7357 - val_loss: 1625344.2500 - val_mae: 1000.8351\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1037274.5000 - mae: 831.0223 - val_loss: 1621247.3750 - val_mae: 1000.9095\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1024071.9375 - mae: 839.2548 - val_loss: 1532799.8750 - val_mae: 1009.7044\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1029360.4375 - mae: 866.5314 - val_loss: 1496355.5000 - val_mae: 1015.8241\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1051265.5000 - mae: 879.7770 - val_loss: 1536562.8750 - val_mae: 1008.5964\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1027041.0625 - mae: 839.5992 - val_loss: 1706310.3750 - val_mae: 994.9184\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1054898.2500 - mae: 831.5378 - val_loss: 1571589.8750 - val_mae: 1004.0070\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1041595.5625 - mae: 840.3118 - val_loss: 1571780.5000 - val_mae: 1003.6964\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1026756.0000 - mae: 856.6360 - val_loss: 1503534.3750 - val_mae: 1012.4387\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1046272.3750 - mae: 862.2676 - val_loss: 1578465.7500 - val_mae: 1002.4398\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1024977.8750 - mae: 846.9005 - val_loss: 1578085.5000 - val_mae: 1002.1772\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1036612.3125 - mae: 844.0768 - val_loss: 1618377.3750 - val_mae: 998.3278\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1027400.6875 - mae: 828.8032 - val_loss: 1647056.1250 - val_mae: 995.8261\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1035099.5000 - mae: 832.9308 - val_loss: 1585406.3750 - val_mae: 1000.6230\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1023587.4375 - mae: 838.7587 - val_loss: 1574296.2500 - val_mae: 1001.3914\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1021682.4375 - mae: 847.3336 - val_loss: 1538419.3750 - val_mae: 1004.9774\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1049490.5000 - mae: 867.2315 - val_loss: 1529985.7500 - val_mae: 1005.7292\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1079433.8750 - mae: 844.6503 - val_loss: 1611945.0000 - val_mae: 997.1781\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1078496.1250 - mae: 886.0114 - val_loss: 1474648.5000 - val_mae: 1024.7983\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1067616.0000 - mae: 887.6414 - val_loss: 1550983.8750 - val_mae: 1002.2856\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1081569.8750 - mae: 825.5126 - val_loss: 1830908.1250 - val_mae: 989.8171\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1088761.8750 - mae: 792.5488 - val_loss: 1686452.0000 - val_mae: 991.6262\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1007069.1250 - mae: 821.4676 - val_loss: 1483403.3750 - val_mae: 1011.8222\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1047714.3750 - mae: 878.4883 - val_loss: 1476522.2500 - val_mae: 1013.5731\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1068472.8750 - mae: 863.7512 - val_loss: 1589088.2500 - val_mae: 997.0441\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1015741.0625 - mae: 833.7546 - val_loss: 1515219.1250 - val_mae: 1004.7878\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1044941.4375 - mae: 872.0438 - val_loss: 1471785.1250 - val_mae: 1014.0952\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1040942.3125 - mae: 877.0960 - val_loss: 1519035.7500 - val_mae: 1003.5689\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1040067.0625 - mae: 843.5610 - val_loss: 1672600.1250 - val_mae: 990.1827\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1037415.6875 - mae: 834.1065 - val_loss: 1535793.8750 - val_mae: 1000.8098\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1014962.7500 - mae: 848.4944 - val_loss: 1565264.8750 - val_mae: 997.3771\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1009749.2500 - mae: 832.3339 - val_loss: 1674685.1250 - val_mae: 989.4384\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1063417.1250 - mae: 809.9890 - val_loss: 1666176.6250 - val_mae: 989.3417\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 995609.6250 - mae: 831.8701 - val_loss: 1465088.7500 - val_mae: 1014.0038\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1149162.6250 - mae: 923.0319 - val_loss: 1476809.7500 - val_mae: 1030.6697\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1097959.8750 - mae: 887.8890 - val_loss: 1609713.3750 - val_mae: 991.9354\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1026387.8750 - mae: 834.1379 - val_loss: 1618572.8750 - val_mae: 990.9117\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1024877.6250 - mae: 814.4243 - val_loss: 1722506.3750 - val_mae: 987.2215\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1047120.3750 - mae: 801.3416 - val_loss: 1579456.0000 - val_mae: 993.5225\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1035400.1250 - mae: 863.2982 - val_loss: 1482047.1250 - val_mae: 1033.5146\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1123403.6250 - mae: 911.7553 - val_loss: 1486567.3750 - val_mae: 1004.0019\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1043767.1875 - mae: 828.7737 - val_loss: 1800428.8750 - val_mae: 985.1426\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1047979.5000 - mae: 814.8408 - val_loss: 1534399.7500 - val_mae: 996.6199\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1013802.9375 - mae: 842.6678 - val_loss: 1520379.3750 - val_mae: 997.9020\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1037223.7500 - mae: 841.2610 - val_loss: 1562961.0000 - val_mae: 993.0388\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1046170.5625 - mae: 865.5016 - val_loss: 1456980.7500 - val_mae: 1011.1911\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1052404.7500 - mae: 880.1196 - val_loss: 1494599.8750 - val_mae: 1000.3423\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1073830.5000 - mae: 834.1415 - val_loss: 1782639.2500 - val_mae: 983.8372\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1037361.2500 - mae: 826.3487 - val_loss: 1485693.6250 - val_mae: 1001.0287\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1016342.3125 - mae: 854.3457 - val_loss: 1512928.6250 - val_mae: 996.6910\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1027520.0000 - mae: 825.7784 - val_loss: 1647690.1250 - val_mae: 985.0575\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1017391.5000 - mae: 807.2274 - val_loss: 1521641.5000 - val_mae: 994.9318\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1014903.8750 - mae: 864.9812 - val_loss: 1451967.1250 - val_mae: 1012.1116\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1057619.7500 - mae: 882.6948 - val_loss: 1508164.5000 - val_mae: 995.8593\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1065116.5000 - mae: 829.6777 - val_loss: 1617033.7500 - val_mae: 985.2140\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1106338.7500 - mae: 877.9666 - val_loss: 1467524.0000 - val_mae: 1027.0240\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1080757.8750 - mae: 892.0773 - val_loss: 1539740.7500 - val_mae: 991.1981\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1044989.3125 - mae: 808.4683 - val_loss: 1845943.5000 - val_mae: 980.4496\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1076727.5000 - mae: 784.4556 - val_loss: 1576601.7500 - val_mae: 987.1137\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 992144.6875 - mae: 832.3824 - val_loss: 1466410.1250 - val_mae: 1000.3391\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1030498.3750 - mae: 866.0038 - val_loss: 1456679.3750 - val_mae: 1002.3939\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1052066.5000 - mae: 877.1843 - val_loss: 1456960.8750 - val_mae: 1001.8332\n"
     ]
    }
   ],
   "source": [
    "#for auxiliary engine\n",
    "# Using dense layer\n",
    "ANNmodel_a = Sequential()\n",
    "ANNmodel_a.add(Dense(128, input_dim=2, activation='relu'))\n",
    "#Output layer\n",
    "ANNmodel_a.add(Dense(1, activation='linear'))\n",
    "\n",
    "ANNmodel_a.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "ANNmodel_a.summary()\n",
    "\n",
    "history = ANNmodel_a.fit(features_train_a, y_train_list_a, validation_split=0.2, epochs =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 128)               384       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513 (2.00 KB)\n",
      "Trainable params: 513 (2.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 49ms/step - loss: 2525646.5000 - mae: 980.3472 - val_loss: 1080727.7500 - val_mae: 786.4441\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1590373.1250 - mae: 868.2102 - val_loss: 1878870.8750 - val_mae: 1035.2313\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 535540.2500 - mae: 483.6576 - val_loss: 113750.7812 - val_mae: 247.9214\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 351814.7812 - mae: 397.2458 - val_loss: 1087048.0000 - val_mae: 777.9153\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 524487.0000 - mae: 493.7248 - val_loss: 63367.6211 - val_mae: 183.6293\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 28426.7637 - mae: 108.3660 - val_loss: 282489.7500 - val_mae: 404.6740\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 230540.5625 - mae: 348.6613 - val_loss: 198525.4062 - val_mae: 340.0969\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 64669.6289 - mae: 154.6361 - val_loss: 68994.2578 - val_mae: 191.8521\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 69734.0078 - mae: 190.7580 - val_loss: 181719.6875 - val_mae: 314.8190\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 76396.5703 - mae: 187.3767 - val_loss: 2618.3879 - val_mae: 32.8552\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 9938.8486 - mae: 63.9973 - val_loss: 66797.7578 - val_mae: 199.4865\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 37987.9414 - mae: 144.9946 - val_loss: 13105.1650 - val_mae: 91.1886\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 3255.9248 - mae: 38.0562 - val_loss: 15495.9863 - val_mae: 87.9106\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 10374.5254 - mae: 67.5718 - val_loss: 7074.8394 - val_mae: 57.4809\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1189.7264 - mae: 19.8040 - val_loss: 2069.7327 - val_mae: 39.0047\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2232.1602 - mae: 38.8361 - val_loss: 2625.0530 - val_mae: 43.3826\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 885.1841 - mae: 24.1507 - val_loss: 532.3237 - val_mae: 13.3441\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 825.6754 - mae: 15.9371 - val_loss: 1260.2350 - val_mae: 21.4161\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 450.1237 - mae: 11.4841 - val_loss: 221.2034 - val_mae: 14.6560\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 352.0595 - mae: 17.4042 - val_loss: 354.2972 - val_mae: 18.0779\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 167.4405 - mae: 11.7225 - val_loss: 261.9371 - val_mae: 9.8615\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 180.7677 - mae: 8.2503 - val_loss: 246.1300 - val_mae: 9.6342\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 85.5905 - mae: 7.4155 - val_loss: 74.8048 - val_mae: 8.1657\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 109.0505 - mae: 10.1974 - val_loss: 94.7191 - val_mae: 9.3544\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 79.5418 - mae: 8.3204 - val_loss: 139.2239 - val_mae: 8.0580\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 91.8838 - mae: 7.3237 - val_loss: 103.2566 - val_mae: 7.7620\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 67.4582 - mae: 7.5930 - val_loss: 72.4331 - val_mae: 8.0120\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 74.7930 - mae: 8.2677 - val_loss: 70.1102 - val_mae: 7.4941\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 61.3206 - mae: 7.2961 - val_loss: 102.8176 - val_mae: 7.7467\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 70.9010 - mae: 7.3834 - val_loss: 80.3512 - val_mae: 7.5136\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 65.3245 - mae: 7.4814 - val_loss: 68.9808 - val_mae: 7.5310\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 64.5451 - mae: 7.5115 - val_loss: 79.6136 - val_mae: 7.5008\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 68.5171 - mae: 7.4229 - val_loss: 71.3996 - val_mae: 7.4485\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 76.6694 - mae: 8.2587 - val_loss: 80.0919 - val_mae: 8.4950\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 73.2174 - mae: 8.0586 - val_loss: 101.7517 - val_mae: 7.7128\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 86.4174 - mae: 7.4011 - val_loss: 75.8310 - val_mae: 7.4622\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 83.9262 - mae: 8.8223 - val_loss: 114.6916 - val_mae: 10.4856\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 86.7744 - mae: 8.9672 - val_loss: 120.1668 - val_mae: 7.8545\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 86.8978 - mae: 7.2144 - val_loss: 106.0546 - val_mae: 7.7344\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 75.0868 - mae: 7.8921 - val_loss: 80.2294 - val_mae: 8.4986\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 74.3432 - mae: 8.2842 - val_loss: 84.2389 - val_mae: 7.4925\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 75.2647 - mae: 7.4572 - val_loss: 82.0334 - val_mae: 7.4578\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 66.9273 - mae: 7.5672 - val_loss: 66.9661 - val_mae: 7.5705\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.9606 - mae: 7.4577 - val_loss: 84.7400 - val_mae: 7.4861\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 64.9241 - mae: 7.3854 - val_loss: 75.0414 - val_mae: 7.4085\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.0069 - mae: 7.3401 - val_loss: 73.7430 - val_mae: 7.3959\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.0691 - mae: 7.2823 - val_loss: 76.1259 - val_mae: 7.4020\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 61.0703 - mae: 7.2625 - val_loss: 69.2064 - val_mae: 7.3529\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 61.8899 - mae: 7.3700 - val_loss: 67.7137 - val_mae: 7.3701\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 61.8334 - mae: 7.4163 - val_loss: 71.2067 - val_mae: 7.3569\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 63.6415 - mae: 7.2822 - val_loss: 80.0895 - val_mae: 7.3962\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.3895 - mae: 7.2478 - val_loss: 68.7880 - val_mae: 7.3268\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.4182 - mae: 7.5030 - val_loss: 65.8970 - val_mae: 7.5408\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 60.6940 - mae: 7.3685 - val_loss: 91.4162 - val_mae: 7.5181\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 76.3383 - mae: 7.3156 - val_loss: 75.6534 - val_mae: 7.3515\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.6343 - mae: 7.3417 - val_loss: 65.5221 - val_mae: 7.4377\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 65.7445 - mae: 7.4158 - val_loss: 67.6531 - val_mae: 7.2929\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 69.1752 - mae: 7.9457 - val_loss: 66.0392 - val_mae: 7.5843\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 60.9364 - mae: 7.3844 - val_loss: 108.5585 - val_mae: 7.6540\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 75.0435 - mae: 7.0439 - val_loss: 65.8060 - val_mae: 7.3211\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 78.8061 - mae: 8.4291 - val_loss: 74.9765 - val_mae: 8.2087\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 68.1237 - mae: 7.6274 - val_loss: 132.1840 - val_mae: 7.8443\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 78.3435 - mae: 7.1521 - val_loss: 66.6021 - val_mae: 7.2503\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.0312 - mae: 7.4478 - val_loss: 64.4334 - val_mae: 7.3873\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 59.4714 - mae: 7.2945 - val_loss: 86.4758 - val_mae: 7.4042\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 67.3804 - mae: 7.1100 - val_loss: 74.8173 - val_mae: 7.2741\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 66.7596 - mae: 7.6782 - val_loss: 63.9997 - val_mae: 7.3831\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 56.0233 - mae: 7.0251 - val_loss: 110.8927 - val_mae: 7.6196\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 70.7041 - mae: 7.0707 - val_loss: 69.9313 - val_mae: 7.2271\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 60.6606 - mae: 7.3472 - val_loss: 64.1348 - val_mae: 7.2678\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 56.6722 - mae: 7.1026 - val_loss: 99.3231 - val_mae: 7.5026\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 70.4917 - mae: 6.9965 - val_loss: 63.7740 - val_mae: 7.4353\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 109.4356 - mae: 10.1010 - val_loss: 80.2437 - val_mae: 8.5619\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 62.7208 - mae: 7.1514 - val_loss: 274.7478 - val_mae: 9.9669\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 109.0092 - mae: 7.1928 - val_loss: 67.5308 - val_mae: 7.7286\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 85.7909 - mae: 8.9942 - val_loss: 63.1162 - val_mae: 7.2372\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 51.1401 - mae: 6.6408 - val_loss: 195.7145 - val_mae: 8.7818\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 93.0151 - mae: 7.4618 - val_loss: 67.8757 - val_mae: 7.7588\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 69.7454 - mae: 7.9678 - val_loss: 70.3710 - val_mae: 7.1600\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 69.2882 - mae: 7.1202 - val_loss: 71.0269 - val_mae: 7.1562\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 69.0541 - mae: 7.8565 - val_loss: 63.3900 - val_mae: 7.4294\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 58.1966 - mae: 7.0161 - val_loss: 118.0112 - val_mae: 7.5871\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 71.0340 - mae: 7.1951 - val_loss: 63.0071 - val_mae: 7.4021\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 59.3383 - mae: 7.2707 - val_loss: 91.3277 - val_mae: 7.3455\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 66.2504 - mae: 6.9807 - val_loss: 69.3329 - val_mae: 7.1083\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 59.1842 - mae: 7.2612 - val_loss: 64.0055 - val_mae: 7.0644\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 55.5324 - mae: 6.9373 - val_loss: 84.5929 - val_mae: 7.2541\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 62.4771 - mae: 7.0489 - val_loss: 62.7920 - val_mae: 7.0480\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 57.0042 - mae: 6.9907 - val_loss: 64.0117 - val_mae: 7.0426\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 56.6435 - mae: 7.0311 - val_loss: 62.4128 - val_mae: 7.0306\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 56.7950 - mae: 7.0100 - val_loss: 69.7951 - val_mae: 7.0635\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 56.8307 - mae: 7.0090 - val_loss: 66.8464 - val_mae: 7.0397\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 58.3178 - mae: 7.1465 - val_loss: 64.1142 - val_mae: 7.0131\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 54.3470 - mae: 6.8953 - val_loss: 81.9380 - val_mae: 7.1781\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 64.1375 - mae: 7.0592 - val_loss: 60.0157 - val_mae: 7.1279\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 60.2765 - mae: 7.3785 - val_loss: 68.2239 - val_mae: 7.0147\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 58.0936 - mae: 6.8610 - val_loss: 68.2076 - val_mae: 7.0066\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 54.6044 - mae: 6.8634 - val_loss: 59.5830 - val_mae: 7.0884\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 56.9667 - mae: 7.1083 - val_loss: 64.7506 - val_mae: 6.9705\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 56.7141 - mae: 6.8521 - val_loss: 66.8930 - val_mae: 6.9752\n"
     ]
    }
   ],
   "source": [
    "#for v app\n",
    "# Using dense layer\n",
    "ANNmodel_v = Sequential()\n",
    "ANNmodel_v.add(Dense(128, input_dim=2, activation='relu'))\n",
    "#Output layer\n",
    "ANNmodel_v.add(Dense(1, activation='linear'))\n",
    "\n",
    "ANNmodel_v.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "ANNmodel_v.summary()\n",
    "\n",
    "history = ANNmodel_v.fit(features_train_v, y_train_list_v, validation_split=0.2, epochs =100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Prediction Based on ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the owner requirements:\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "\n",
      "So here are the prediction of Engine Power and Vapp based on DWT : 18200 tonnes and Vs : 16.0 knots :\n",
      "Main Engine Power :  5735.693359375 Kw\n",
      "Aux Engine Power :  556.2263793945312 Kw\n",
      "Vapp :  8.01 Knots\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('This is the owner requirements:')\n",
    "capacity_input = int(input('The capacity in DWT is:'))\n",
    "speed_input = float(input('The service speed in knots is:'))\n",
    "prediction_v = ANNmodel_v.predict([[capacity_input,speed_input]])\n",
    "prediction_v_score = float(prediction_v.item(0))\n",
    "prediction_m = ANNmodel_m.predict([[capacity_input,speed_input]])\n",
    "prediction_m_score = float(prediction_m.item(0))\n",
    "prediction_a = ANNmodel_a.predict([[capacity_input,speed_input]])\n",
    "prediction_a_score = float(prediction_a.item(0))\n",
    "print('')\n",
    "print('So here are the prediction of Engine Power and Vapp based on DWT :',capacity_input,'tonnes and Vs :',speed_input,'knots :')\n",
    "print('Main Engine Power : ',prediction_m_score,'Kw')\n",
    "print('Aux Engine Power : ',prediction_a_score,'Kw')\n",
    "print('Vapp : ',round(prediction_v_score,2),'Knots')\n",
    "print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
